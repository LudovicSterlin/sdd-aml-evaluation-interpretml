{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La Latent Dirichlet Allocation est un \"modèle statistique génératif\" : c'est une méthode qui permet de générer des groupes qui permettent d'expliquer statistiquement certaines ressemblances dans un ensemble de données. Utilisée comme méthode de Natural Language Processing, la LDA permet de modéliser des thèmes récurrents dans ces textes (*topic modelling* en anglais). En pratique, utiliser la LDA permet de :\n",
    "- modéliser le corpus d'entrée de manière différente de BOW et Word2Vec\n",
    "- générer des thèmes du corpus de texte (un thème = un ensemble de mots reliés sémantiquement au même sujet)\n",
    "\n",
    "<img src=\"TopicModelling.png\" width=600>\n",
    "\n",
    "On va donc d'abord [comprendre la théorie](#sec1.), puis [s'intéresser à son application pratique](#sec2.), et enfin [comparer ces résultats](#sec3.) avec une autre méthode de *topic modelling* : la Nonnegative Matrix Factorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sommaire :\n",
    "\n",
    "* I\\. [En théorie](#sec1.)<br>\n",
    "    * 1. [But de la méthode](#sec1.1.)<br>\n",
    "    * 2. [TLDR](#sec1.2.)<br>\n",
    "    * 3. [Quelques détails](#sec1.3.)<br>\n",
    "* II\\. [En pratique](#sec2.)<br>\n",
    "    * 1. [Récupération des données](#sec2.1.)<br>\n",
    "    * 2. [BOW et TFIDF](#sec2.2.)<br>\n",
    "    * 3. [Latent Dirichlet Allocation](#sec2.3.)<br>\n",
    "    * 4. [Comparaison des représentations](#sec2.4.)<br>\n",
    "    * 5. [Une autre méthode de Topic Modelling : la NMF](#sec2.5.)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"sec1.\"></a> 1. En théorie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La mise en place de la LDA nécessite des notations de statistique bayésienne assez lourdes. On donne ici une présentation simple pour comprendre grossièrement le principe et les notations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"sec1.1.\"></a> 1.1. But de la méthode\n",
    "Le but de la méthode est de pouvoir associer à chaque mot de chaque document la probabilité qu'il soit issu d'un thème."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"sec1.2.\"></a> 1.2. TLDR\n",
    "Pour résumer le principe de la méthode en quelques lignes :\n",
    "\n",
    "- La distribution a priori des thèmes au sein de chaque document est une loi de Dirichlet\n",
    "- Ensuite, pour chaque mot du corpus, on met à jour la loi a posteriori (i.e. \"au vu des données\") en fixant les distributions de tous les autres mots et en calculant les nouvelles distributions du mot considéré via formule de Bayes et intégrations. \n",
    "\n",
    "Il y a plusieurs distributions à considérer : la probabilité qu'un document soit assigné à un thème, qu'un mot soit issu d'un thème... Les calculs sont difficiles et nécessitent des approximations fines.\n",
    "\n",
    "Cela permet aussi de comprendre le nom de la méthode : \"Dirichlet\" car c'est la distribution a priori thème-document et \"Latent\" car on réalise les calculs dans l'\"espace des thèmes\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"sec1.3.\"></a> 1.3. Quelques détails\n",
    "Quelques détails histoire de retirer l'aspect \"boîte noire\" (non-nécessaires pour la mise en pratique) : \n",
    "### Initialisation\n",
    "On choisit d'abord le nombre de thèmes à générer, noté $T$. Pour chaque document $d \\in \\{1, ..., D\\}$ du corpus, on génère $\\theta_d \\sim Dir(\\alpha)$ (loi de Dirichlet) avec $\\alpha \\in ]0, 1[^T$ : \n",
    "\n",
    "- Chaque élément $\\alpha_t$ de $\\alpha$ correspond au poids a priori du thème t dans un document quelconque.\n",
    "- La loi de Dirichlet est une sorte de \"loi bêta en N-dimensions\". C'est un choix qui permet de faciliter les calculs : cette distribution est conjuguée* à la loi multinomiale (binomial en N-dimension) qui est elle-même utilisée dans les calculs. C'est cette loi qui donne son nom à la méthode.\n",
    "\n",
    "<img src=\"Dirichlet.png\" width=600>\n",
    "<center><b>Exemple de distribution de Dirichlet (3D)</b></center>\n",
    "\n",
    "- $\\theta_d \\in [0, 1]^T$ représente les probabilités que chaque thème t apparaisse dans le document d. \n",
    "\n",
    "D'autre part, on fixe $\\beta \\in \\mathcal{M}_{T, N}([0, 1])$ où N est le nombre total de mots : $\\beta_{i, j} = \\mathbb{P}$(\"Le mot d'indice j est issu du thème d'indice i\"). Le but est ainsi d'estimer $\\beta$.\n",
    "\n",
    "\\*sans rentrer dans le détail, une loi a priori est conjuguée à la loi du modèle expérimental (\"vraisemblance\") si la loi a posteriori (i.e. \"au vu des données\") a la même forme que la loi a priori. Il n'y a donc aucun calcul à réaliser dans ce cas !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apprentissage\n",
    "Les probabilités ainsi initialisées sont complètement aléatoires. On améliore petit à petit le modèle jusqu'à stabilisation des distributions. Les notations sont simplifiées (presque erronées...) à but pédagogique.\n",
    "\n",
    "Pour chaque mot $m$, chaque document $d$ et chaque thème $t$, on calcule :\n",
    "\n",
    "- $p(t\\mid d)$ la probabilité que le document $d$ soit assigné au thème $t$\n",
    "- $p(w\\mid t)$ la probabilité que le thème $t$ soit assigné au mot $w$\n",
    "\n",
    "On peut alors calculer la probabilité que le thème $t$ génère $w$ dans le document $d$ via une intégrale du produit de ces probabilités. Il faut noter que ce dernier calcul (l'inférence) est très loin d'être trivial et nécessite des méthodes de calcul assez lourdes pour obtenir une bonne approximation (souvent par méthode \"variationnelle bayésienne\", pour citer son nom).\n",
    "\n",
    "Une fois que les distributions sont stables, chaque thème est constitué des mots dont la probabilité d'être d'en être issu est parmi les plus fortes : un mot peut donc appartenir à plusieurs thèmes.\n",
    "\n",
    "Pour plus de détails, le papier originel explique les différents calculs de probabilités : http://ai.stanford.edu/~ang/papers/jair03-lda.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"sec2.\"></a> 2. En pratique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La LDA s'utilise dans un contexte de NLP. Pour suivre le fil rouge de NLP *spam or ham*, on peut utiliser la LDA pour :\n",
    "\n",
    "- modéliser le corpus d'entrée de manière différente de BOW et Word2Vec et comparer les résultats de classification\n",
    "- observer les thèmes récurrents dans les spams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"sec2.1.\"></a> 2. 1. Récupération des données\n",
    "\n",
    "D'abord, on va chercher les données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Petit rappel : 2893 emails dont 481 spams\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def get_emails(train_dir):\n",
    "    email_path = []\n",
    "    email_label = []\n",
    "    for d in os.listdir(train_dir):\n",
    "        folder = os.path.join(train_dir,d)\n",
    "        email_path += [os.path.join(folder,f) for f in os.listdir(folder)]\n",
    "        email_label += [f[0:3]=='spm' for f in os.listdir(folder)]\n",
    "    return email_path, email_label\n",
    "\n",
    "train_dir = '../data/lingspam_public/bare/' # Mettre le chemin correct s'il ne fonctionne pas\n",
    "email_path, email_label = get_emails(train_dir)\n",
    "\n",
    "print(f\"Petit rappel : {len(email_path)} emails dont {np.sum(email_label)} spams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On crée un tokenizer qui retire tous les mots inutiles à la classification :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import wordpunct_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self, stop_words = None, remove_non_words=True):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        if stop_words is None:\n",
    "            self.stopwords = set(stopwords.words('english'))\n",
    "        else:\n",
    "            self.stopwords = stop_words\n",
    "        self.words = set(words.words())\n",
    "        self.remove_non_words = remove_non_words\n",
    "    def __call__(self, doc):\n",
    "        # tokenize words and punctuation\n",
    "        word_list = wordpunct_tokenize(doc)\n",
    "        # remove stopwords\n",
    "        word_list = [word for word in word_list if word not in self.stopwords]\n",
    "        # remove non words\n",
    "        if(self.remove_non_words):\n",
    "            word_list = [word for word in word_list if word in self.words]\n",
    "        # remove 1-character words\n",
    "        word_list = [word for word in word_list if len(word)>1]\n",
    "        # remove non alpha\n",
    "        word_list = [word for word in word_list if word.isalpha()]\n",
    "        return [self.wnl.lemmatize(t) for t in word_list]\n",
    "    \n",
    "#L'outil qui \"compte les mots\"\n",
    "countvect = CountVectorizer(input='filename',tokenizer=LemmaTokenizer(remove_non_words=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"sec2.2.\"></a> 2. 2. BOW et TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient la représentation Bag Of Words correspondante (avec TFIDF) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#Petit rappel : le BOW est une matrice de taille (nb_documents, nb_mots)\n",
    "bow = countvect.fit_transform(email_path)\n",
    "#On relie la position dans le BOW au vrai mot grâce au dictionnaire\n",
    "feat2word = {v: k for k, v in countvect.vocabulary_.items()}\n",
    "#On pondère avec TFIDF (cf le cours sur le NLP)\n",
    "X_tfidf = TfidfTransformer().fit_transform(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"sec2.3.\"></a> 2. 3. Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En pratique, on réalise la LDA à partir du BOW. La transformation TFIDF ne donne en pratique pas de bons résultats car la division par la \"fréquence de document\" retire une information essentielle pour la LDA. On peut en revanche pondérer par la TF. En supposant que l'on veut déterminer 4 thèmes dans tous les emails :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "n_topics = 3\n",
    "tf = TfidfTransformer(use_idf = False).fit_transform(bow)\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, #nombre de thèmes, T précedemment\n",
    "                                doc_topic_prior=None,  #précédemment alpha, par défaut 1/n_topics\n",
    "                                topic_word_prior=None, #précédemment beta_i,j, par défaut 1/n_topics\n",
    "                                max_iter=5, learning_method='online')\n",
    "lda.fit(tf);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut alors afficher les mots du corpus qui correspondent le mieux à chaque thème obtenu :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: cloven crossword cleaver cleave divide hoof meat adhere yeni photography subject psoriasis sculpture tantalize tempt\n",
      "Topic #1: cream syrup subject squash dope precinct fluff relish rumble collard bagel baked muffin tuna chipmunk\n",
      "Topic #2: university language subject linguistics information mail one please de conference new would address research also\n"
     ]
    }
   ],
   "source": [
    "def print_topics(model, feature_names, n_words):\n",
    "    '''Affiche les n_words mots les plus probables pour chaque thème pour le modèle donné (LDA... ou NMF!)'''\n",
    "    #topic_idx = 0, 1, 2, 3...\n",
    "    #topic est la liste des probas que chaque mot soit issu du thème noté \"topic_idx\"\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        #topic.argsort()[:-n_words - 1:-1]] simply gives the indexes of most likely words for topic\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_words - 1:-1]])\n",
    "        print(message)\n",
    "    return\n",
    "\n",
    "feature_names = countvect.get_feature_names()\n",
    "print_topics(lda, feature_names, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien que les thèmes sont a priori \"théoriques\", on peut retrouver une sémantique à travers ces thèmes (pas forcément dans cet ordre car l'ordre de création des thèmes est aléatoire) : \n",
    "\n",
    "- le thème 0 regroupe des termes autour du clivage (cloven, cleaver, divide, adhere...)\n",
    "- le thème 1 regroupe des termes autour du langage et de l'académique (university, language, subject, information, research...)\n",
    "- le thème 2 regroupe des termes autour de la cuisine (cream, syrup, sauce, cranberry, pantry, whipped...)\n",
    "\n",
    "La représentation correspondante d'un document est donc un vecteur de taille n_topics dont chaque composante correspond à la probabilité d'être généré par un des thèmes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avec 2893 mails et 3 thèmes, les emails sont représentés par une matrice de taille (2893, 3)\n",
      "Le premier document est représenté par [0.05064384 0.89870755 0.05064861]\n"
     ]
    }
   ],
   "source": [
    "X_lda = lda.fit_transform(tf)\n",
    "print(f\"Avec {tf.shape[0]} mails et {n_topics} thèmes, les emails sont représentés par une matrice de taille {X_lda.shape}\")\n",
    "print(f\"Le premier document est représenté par {X_lda[0, :]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"sec2.4.\"></a> 2.4. Comparaison des représentations\n",
    "On peut alors comparer les performances de ces représentations vis-à-vis d'une classification. On utilise par exemple les random forests pour l'étape de classification. D'abord TFIDF :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score F1 des Random Forests avec TFIDF :  0.93 (+/-  0.02)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, criterion='entropy')\n",
    "n_splits = 4\n",
    "\n",
    "#Validation croisée stratifiée (on conserve les proportions de classe)\n",
    "cv = StratifiedKFold(n_splits = n_splits, shuffle=True)\n",
    "\n",
    "#Scores f1 en validation croisée (plus pertinent que la précision lorsque le dataset n'est pas équilibré)\n",
    "scores = cross_val_score(clf, X_tfidf, email_label, cv=cv, scoring = 'f1')\n",
    "\n",
    "print(f\"Score F1 des Random Forests avec TFIDF : {scores.mean() : .2f} (+/- {scores.std() * 2 : .2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score F1 des Random Forests avec LDA : 0.22 (+/- 0.05)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, criterion='entropy')\n",
    "n_splits = 4\n",
    "\n",
    "#Validation croisée stratifiée (on conserve les proportions de classe)\n",
    "cv = StratifiedKFold(n_splits = n_splits, shuffle=True)\n",
    "\n",
    "#Scores f1 en validation croisée (plus pertinent que la précision lorsque le dataset n'est pas équilibré)\n",
    "scores = cross_val_score(clf, X_lda, email_label, cv=cv, scoring = 'f1')\n",
    "\n",
    "print(f\"Score F1 des Random Forests avec LDA ({n_topics} thèmes) : %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sans grande surprise, on voit que la représentation TFIDF est de loin la meilleure pour la classification de spam. Les piètres performances de la LDA s'expliquent aisément : on tente dans ce cas de déterminer si un e-mail est un spam à partir de regroupements sémantiques; or un thème particulier peut aussi bien apparaître dans un mail normal que dans un spam (par exemple la \"cuisine\", thème observé précédemment). Même en augmentant le nombre de thèmes, on n'obtient pas de meilleurs résultats.\n",
    "\n",
    "Plus généralement, la LDA est efficace lorsque ces groupes créés permettent d'expliquer la variable observée :\n",
    "\n",
    "- En Biologie, cette méthode permet de détecter la présence d'une variation génétique *structurelle* au sein d'un groupe d'individus\n",
    "- En Natural Language Processing, la LDA est souvent efficace pour la Sentiment Analysis : le regroupement par thèmes permet de déterminer si un texte est plutôt \"positif\" ou \"négatif\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"sec2.5.\"></a> 2.5. Une autre méthode de Topic Modelling : la NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sans rentrer dans le détail, il existe d'autres méthodes de Topic Modelling. On présente ici en pratique  la Nonnegative Matrix Factorization. Il s'agit initialement d'une méthode de factorisation matricielle permettant la décomposition d'une matrice quelconque V :\n",
    "$$V = W \\times H $$\n",
    "où H et W ont des coefficients uniquements positifs.\n",
    "<img src=\"NMF.jpg\" width=600>\n",
    "<center><b>Décomposition approximative NMF</b></center>\n",
    "\n",
    "Cette méthode a des propriétés inhérentes de partitionnement : \n",
    "- en imposant H orthogonal, la méthode a une formulation mathématique équivalente à la méthode \"K-means\"\n",
    "- avec une métrique particulière (divergence de Kullback–Leibler, très utilisée en statistique bayésienne), elle correspond à la méthode nommée pLSA, méthode dont est issue la LDA.\n",
    "\n",
    "En appliquant cette factorisation à la matrice TFIDF, on peut donner un sens à W et H. On nomme T la dimension \"latente\" :\n",
    "- V est de dimensions (D, N) : D documents pour N mots au total\n",
    "- W est de dimensions (D, T) : D documents pour T \"thèmes\". W relie donc les documents aux \"thèmes\" latents.\n",
    "- H est de dimensions (T, N) : T thèmes pour N mots. H relie donc les thèmes aux mots.\n",
    "(consulter [ce lien](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html) pour plus de détails)\n",
    "\n",
    "On peut observer ce que cela donne sur les emails :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: report mail program money people order make send name get work business time one address\n",
      "Topic #1: language university conference linguistics de information session workshop research one may subject linguistic paper new\n",
      "Topic #2: link directory web free net page search mail order world index add business offer best\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "n_topics = 3\n",
    "nmf = NMF(n_components=n_topics).fit(bow)\n",
    "feature_names = countvect.get_feature_names()\n",
    "print_topics(nmf, feature_names, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que cette méthode donne des thèmes sensiblement différents de ceux obtenus avec LDA (bien que l'on retrouve un thème sur le langage et l'académique). Lors de l'utilisation de la LDA, on pourra donc comparer ses résultats avec ceux de la NMF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"secconclusion\"></a>Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir expliqué théoriquement la Latent Dirichlet Allocation, on en a présenté une application pratique sur le cas fil rouge *spam or ham*. Ce cas de classification est peu propice à la LDA car n'utilise pas correctement le partitionnement en \"thèmes\". En revanche, la LDA est très efficace lorsque les \"thèmes\" expliquent correctement la variable cible, comme en Sentiment Analysis par exemple. On a ensuite rapidement décrit la méthode Nonnegative Matrix Factorization comme alternative à la LDA : les thèmes obtenus sont différents de ceux générés par LDA et on pourra donc comparer les résultats de la NMF avec ceux de la LDA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
