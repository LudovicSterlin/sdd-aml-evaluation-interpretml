{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision trees in Julia\n",
    "\n",
    "*Alexandre Slonina*  \n",
    "Subject link: https://github.com/bensadeghi/DecisionTree.jl\n",
    "\n",
    "### First Prerequiste: install julia\n",
    "\n",
    "You can run this notebook on google colab with the solution explained here [Julia for Pythonistas](https://github.com/ageron/julia_notebooks/blob/master/Julia_for_Pythonistas.ipynb) but personaly I would not recommend it.\n",
    "\n",
    "To install julia with anaconda:  \n",
    "`$ conda install -c conda-forge julia`\n",
    "\n",
    "If it doesn't work download julia manualy:  \n",
    "`$ sudo apt install julia` (Linux) or https://julialang.org/downloads\n",
    "\n",
    "Launch Julia:  \n",
    "`$ julia`\n",
    "\n",
    "Follow these lines:\n",
    "- `using Pkg`\n",
    "- `Pkg.add(\"IJulia\")`\n",
    "\n",
    "Julia should now be available on your conda environment ! \n",
    "You must have \"Julia 1.X.X\" instead of \"Python 3\" on the top right of your screen, if not, try to re launch jupyter.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, why using Julia instead of Python ?\n",
    "\n",
    "Sorry Dennis spoiled you during the last OBD course ...  \n",
    "Julia is a new language created in 2012 especially for Data Science and linear algebra. Quickly the advantages of Julia are : \n",
    "- The syntax is optimized for math and machine learning\n",
    "- Speed (it tries to imitate C) \n",
    "- Native machine learning libraries\n",
    "\n",
    "Many Data Scientists keep using python beacause it's more popular yet and there are more third-party packages (PyTorch, TensorFlow are the main ones).\n",
    "\n",
    "If you are interested in the subject :  \n",
    "[Link1: datascientest.com](https://datascientest.com/python-vs-julia-quel-est-le-meilleur-langage-pour-la-data-science)  \n",
    "[Link2: geeksforgeeks.org](https://www.geeksforgeeks.org/julia-vs-python/)   \n",
    "[Link3: analyticsvidhya.com](https://www.analyticsvidhya.com/blog/2020/08/what-is-better-for-data-science-learning-and-work-julia-or-python/) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some important differences\n",
    "  * Arrays in Julia are indexed starting from 1.\n",
    "  * In Julia classes (i.e. types) don't own methods. Methods are implementations of generic functions and are invoked in a \"static style\", i.e. instead of Python's str1.rstrip(), we will have rstrip( str1 ), instead of file1.close(), close( file1 ).\n",
    "\n",
    "### Some important similarities.\n",
    "\n",
    "\n",
    "| *Python*                 | *Julia* | *Comments* |\n",
    "| -------------------------|---------|----------|\n",
    "| `True` | `true` | |\n",
    "| `False` | `false` | | \n",
    "| `None` | `nothing` |  |\n",
    "| `type( obj )` | `typeof( obj )` |  |\n",
    "| `{}` | `Dict{KeyType,ValueType}()` |  |\n",
    "| `elif` | `elseif` |  |\n",
    "| `lambda x, y : y + x * 2` | `(x,y) -> y + x * 2` |  |\n",
    "| `\"string %s interpolation %d\" % ( str1, i1)` | `\"string $str1 interpolation $i1 \"` | You can interpolate arbitrary expressions by enclosing them in braces, as in `\"${x+y}\"` | \n",
    "| `xrange(10,4,-2`)` | `10:-2:4` | |\n",
    "| `range(10,4,-2)` | `[10:-2:4]` | Do this only if you really have to, as it will consume memory proportional to the length of the range | \n",
    "| `id( obj )` | `object_id( obj )` | | \n",
    "| `raise excetion` | `throw( exception )` |\n",
    "\n",
    "\n",
    "| *Python*                 | *Julia* |\n",
    "| -------------------------|-----------|\n",
    "| `str1 + str2 + str2` |  `string( str1, str2, str3 )`  |\n",
    "| `len( str1 )` |  `length( str1 )` | \n",
    "| `str1.rstrip()` | `rstrip( str1 )` | \n",
    " \n",
    "[Source](https://gist.github.com/svaksha/bf2b287e85967dcaad03a26d8b1e523d)  \n",
    "To go further: [Julia official doc](https://docs.julialang.org/en/v1/manual/noteworthy-differences/#Noteworthy-differences-from-Python)\n",
    "\n",
    "For those who are interested in Julia there are many tutorials to getting started.  \n",
    "[JuliaAcademy](https://juliaacademy.com/courses/)\n",
    "[GithubRepository](https://github.com/JuliaAcademy/JuliaTutorials)\n",
    "\n",
    "Here is the notebook for pythonistas with all you need: [Julia for Pythonistas](https://github.com/ageron/julia_notebooks/blob/master/Julia_for_Pythonistas.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick reminder of Decision Trees\n",
    "\n",
    "**Hierarchical description of data based on logical (binary) questions**.  \n",
    "Basic Idea: Test the attributes (features) sequentially\n",
    "= Ask questions about the target/status sequentially\n",
    "\n",
    "Ask about the attribute which maximizes the expected\n",
    "reduction of the entropy.\n",
    "\n",
    "Ingredients:\n",
    "- Nodes<br>\n",
    "Each node contains a **test** on the features which **partitions** the data.\n",
    "- Edges<br>\n",
    "The outcome of a node's test leads to one of its child edges.\n",
    "- Leaves<br>\n",
    "A terminal node, or leaf, holds a **decision value** for the output variable.\n",
    "\n",
    "![Tree example](img/dt1.png)\n",
    "\n",
    "If you want to get back in the math behind you can quickly run the MLclass notebook : [8-Decision Trees](https://github.com/erachelson/MLclass/tree/master/8%20-%20Decision%20Trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble methods used with Decision Trees\n",
    "\n",
    "\n",
    "**Boosting:** A set of weak learners creates a single strong\n",
    "classifier. Apply learner to weighted samples and then Increase weights of misclassified examples.  \n",
    "[9-Boosting](https://github.com/erachelson/MLclass/tree/master/9%20-%20Boosting)\n",
    "\n",
    "**Bagging:** (Bootstrap Aggregating) Use bootstrap replicates of training set by sampling with replacement. On each replicate learn one model. This lead to High variance and low bias classifiers. Combined altogether it reduces the variance of the classifier. It's a powerful algorithm for controling overfitting.  \n",
    "[10-Bagging](https://github.com/erachelson/MLclass/tree/master/10%20-%20Bagging)  \n",
    "\n",
    "**Forest:** Bagging + Random feature selection at each node   \n",
    "[11-Random Forest](https://github.com/erachelson/MLclass/tree/master/11%20-%20Random%20Forests)\n",
    "\n",
    "Visual examples for boosting and bagging are [here](ressources/10-ensemble-6.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second prerequiste: install packages\n",
    "\n",
    "The keyword **import** in python is **using** in julia. To install the \"example\" package in julia you have to run Pkg.add(\"example\") it's like a pip install \"example\" for python. Once it's done you'll never have to do it again on your machine. So please uncomment the lines below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "using Pkg\n",
    "#Pkg.add(\"DecisionTree\")\n",
    "#Pkg.add(\"ScikitLearn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple classification example using Iris dataset\n",
    "The API is quite the same as scikitlearn in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we import the package\n",
    "using DecisionTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This package support the ScikitLearn interface. The Available models are: `DecisionTreeClassifier, DecisionTreeRegressor, RandomForestClassifier, RandomForestRegressor, AdaBoostStumpClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mD\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mT\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mC\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mf\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "DecisionTreeClassifier(; pruning_purity_threshold=0.0,\n",
       "                       max_depth::Int=-1,\n",
       "                       min_samples_leaf::Int=1,\n",
       "                       min_samples_split::Int=2,\n",
       "                       min_purity_increase::Float=0.0,\n",
       "                       n_subfeatures::Int=0,\n",
       "                       rng=Random.GLOBAL_RNG)\n",
       "\\end{verbatim}\n",
       "Decision tree classifier. See \\href{https://github.com/bensadeghi/DecisionTree.jl}{DecisionTree.jl's documentation}\n",
       "\n",
       "Hyperparameters:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{pruning\\_purity\\_threshold}: (post-pruning) merge leaves having \\texttt{>=thresh} combined purity (default: no pruning)\n",
       "\n",
       "\n",
       "\\item \\texttt{max\\_depth}: maximum depth of the decision tree (default: no maximum)\n",
       "\n",
       "\n",
       "\\item \\texttt{min\\_samples\\_leaf}: the minimum number of samples each leaf needs to have (default: 1)\n",
       "\n",
       "\n",
       "\\item \\texttt{min\\_samples\\_split}: the minimum number of samples in needed for a split (default: 2)\n",
       "\n",
       "\n",
       "\\item \\texttt{min\\_purity\\_increase}: minimum purity needed for a split (default: 0.0)\n",
       "\n",
       "\n",
       "\\item \\texttt{n\\_subfeatures}: number of features to select at random (default: keep all)\n",
       "\n",
       "\n",
       "\\item \\texttt{rng}: the random number generator to use. Can be an \\texttt{Int}, which will be used to seed and create a new random number generator.\n",
       "\n",
       "\\end{itemize}\n",
       "Implements \\texttt{fit!}, \\texttt{predict}, \\texttt{predict\\_proba}, \\texttt{get\\_classes}\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "DecisionTreeClassifier(; pruning_purity_threshold=0.0,\n",
       "                       max_depth::Int=-1,\n",
       "                       min_samples_leaf::Int=1,\n",
       "                       min_samples_split::Int=2,\n",
       "                       min_purity_increase::Float=0.0,\n",
       "                       n_subfeatures::Int=0,\n",
       "                       rng=Random.GLOBAL_RNG)\n",
       "```\n",
       "\n",
       "Decision tree classifier. See [DecisionTree.jl's documentation](https://github.com/bensadeghi/DecisionTree.jl)\n",
       "\n",
       "Hyperparameters:\n",
       "\n",
       "  * `pruning_purity_threshold`: (post-pruning) merge leaves having `>=thresh` combined purity (default: no pruning)\n",
       "  * `max_depth`: maximum depth of the decision tree (default: no maximum)\n",
       "  * `min_samples_leaf`: the minimum number of samples each leaf needs to have (default: 1)\n",
       "  * `min_samples_split`: the minimum number of samples in needed for a split (default: 2)\n",
       "  * `min_purity_increase`: minimum purity needed for a split (default: 0.0)\n",
       "  * `n_subfeatures`: number of features to select at random (default: keep all)\n",
       "  * `rng`: the random number generator to use. Can be an `Int`, which will be used to seed and create a new random number generator.\n",
       "\n",
       "Implements `fit!`, `predict`, `predict_proba`, `get_classes`\n"
      ],
      "text/plain": [
       "\u001b[36m  DecisionTreeClassifier(; pruning_purity_threshold=0.0,\u001b[39m\n",
       "\u001b[36m                         max_depth::Int=-1,\u001b[39m\n",
       "\u001b[36m                         min_samples_leaf::Int=1,\u001b[39m\n",
       "\u001b[36m                         min_samples_split::Int=2,\u001b[39m\n",
       "\u001b[36m                         min_purity_increase::Float=0.0,\u001b[39m\n",
       "\u001b[36m                         n_subfeatures::Int=0,\u001b[39m\n",
       "\u001b[36m                         rng=Random.GLOBAL_RNG)\u001b[39m\n",
       "\n",
       "  Decision tree classifier. See DecisionTree.jl's documentation\n",
       "  (https://github.com/bensadeghi/DecisionTree.jl)\n",
       "\n",
       "  Hyperparameters:\n",
       "\n",
       "    •    \u001b[36mpruning_purity_threshold\u001b[39m: (post-pruning) merge leaves having\n",
       "        \u001b[36m>=thresh\u001b[39m combined purity (default: no pruning)\n",
       "\n",
       "    •    \u001b[36mmax_depth\u001b[39m: maximum depth of the decision tree (default: no\n",
       "        maximum)\n",
       "\n",
       "    •    \u001b[36mmin_samples_leaf\u001b[39m: the minimum number of samples each leaf needs to\n",
       "        have (default: 1)\n",
       "\n",
       "    •    \u001b[36mmin_samples_split\u001b[39m: the minimum number of samples in needed for a\n",
       "        split (default: 2)\n",
       "\n",
       "    •    \u001b[36mmin_purity_increase\u001b[39m: minimum purity needed for a split (default:\n",
       "        0.0)\n",
       "\n",
       "    •    \u001b[36mn_subfeatures\u001b[39m: number of features to select at random (default:\n",
       "        keep all)\n",
       "\n",
       "    •    \u001b[36mrng\u001b[39m: the random number generator to use. Can be an \u001b[36mInt\u001b[39m, which will\n",
       "        be used to seed and create a new random number generator.\n",
       "\n",
       "  Implements \u001b[36mfit!\u001b[39m, \u001b[36mpredict\u001b[39m, \u001b[36mpredict_proba\u001b[39m, \u001b[36mget_classes\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150-element Array{String,1}:\n",
       " \"Iris-setosa\"\n",
       " \"Iris-setosa\"\n",
       " \"Iris-setosa\"\n",
       " \"Iris-setosa\"\n",
       " \"Iris-setosa\"\n",
       " \"Iris-setosa\"\n",
       " \"Iris-setosa\"\n",
       " \"Iris-setosa\"\n",
       " \"Iris-setosa\"\n",
       " \"Iris-setosa\"\n",
       " \"Iris-setosa\"\n",
       " \"Iris-setosa\"\n",
       " \"Iris-setosa\"\n",
       " ⋮\n",
       " \"Iris-virginica\"\n",
       " \"Iris-virginica\"\n",
       " \"Iris-virginica\"\n",
       " \"Iris-virginica\"\n",
       " \"Iris-virginica\"\n",
       " \"Iris-virginica\"\n",
       " \"Iris-virginica\"\n",
       " \"Iris-virginica\"\n",
       " \"Iris-virginica\"\n",
       " \"Iris-virginica\"\n",
       " \"Iris-virginica\"\n",
       " \"Iris-virginica\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features, labels = load_data(\"iris\")\n",
    "\n",
    "# the data loaded are of type Array{Any}\n",
    "# cast them to concrete types for better performance\n",
    "features = float.(features)\n",
    "labels   = string.(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 3, Threshold 2.45\n",
      "L-> Iris-setosa : 50/50\n",
      "R-> Feature 4, Threshold 1.75\n",
      "    L-> Iris-versicolor : 49/54\n",
      "    R-> Iris-virginica : 45/46\n"
     ]
    }
   ],
   "source": [
    "# train depth-truncated classifier\n",
    "model = DecisionTreeClassifier(max_depth=2)\n",
    "fit!(model, features, labels)\n",
    "# pretty print of the tree, to a depth of 3 nodes (usefull if you change the max_depth above)\n",
    "print_tree(model,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the same result as in python, here was the result for the same tree running the 9nth notebook\n",
    "![dt2](img/iris_dt2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.9607843137254902\n",
       " 0.9019607843137255\n",
       " 0.9791666666666666"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run n-fold cross validation over 3 CV folds\n",
    "# See ScikitLearn.jl for installation instructions\n",
    "using ScikitLearn.CrossValidation: cross_val_score\n",
    "accuracy = cross_val_score(model, features, labels, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myIris type is Iris-virginica\n",
      "[0.0, 0.021739130434782608, 0.9782608695652174]\n",
      "[\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"]\n"
     ]
    }
   ],
   "source": [
    "# let's say we have some features corresponding to an iris, which type is it ?\n",
    "myIris = [5.9,3.0,5.1,1.9]\n",
    "# apply learned model\n",
    "print(\"myIris type is \")\n",
    "println(predict(model, myIris))\n",
    "# get the probability of each label\n",
    "println(predict_proba(model, myIris))\n",
    "println(get_classes(model)) # returns the ordering of the columns in predict_proba's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Array{Int64,2}:\n",
       " 50   0   0\n",
       "  0  49   1\n",
       "  0   5  45"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Classes:  [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"]\n",
       "Matrix:   \n",
       "Accuracy: 0.96\n",
       "Kappa:    0.94"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = predict(model, features)\n",
    "confusion_matrix(labels, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest\n",
    "It also works with random forest, the syntax is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mR\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1md\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mm\u001b[22m\u001b[0m\u001b[1mF\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mC\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mf\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "RandomForestClassifier(; n_subfeatures::Int=-1,\n",
       "                       n_trees::Int=10,\n",
       "                       partial_sampling::Float=0.7,\n",
       "                       max_depth::Int=-1,\n",
       "                       rng=Random.GLOBAL_RNG)\n",
       "\\end{verbatim}\n",
       "Random forest classification. See \\href{https://github.com/bensadeghi/DecisionTree.jl}{DecisionTree.jl's documentation}\n",
       "\n",
       "Hyperparameters:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{n\\_subfeatures}: number of features to consider at random per split (default: -1, sqrt(\\# features))\n",
       "\n",
       "\n",
       "\\item \\texttt{n\\_trees}: number of trees to train (default: 10)\n",
       "\n",
       "\n",
       "\\item \\texttt{partial\\_sampling}: fraction of samples to train each tree on (default: 0.7)\n",
       "\n",
       "\n",
       "\\item \\texttt{max\\_depth}: maximum depth of the decision trees (default: no maximum)\n",
       "\n",
       "\n",
       "\\item \\texttt{min\\_samples\\_leaf}: the minimum number of samples each leaf needs to have\n",
       "\n",
       "\n",
       "\\item \\texttt{min\\_samples\\_split}: the minimum number of samples in needed for a split\n",
       "\n",
       "\n",
       "\\item \\texttt{min\\_purity\\_increase}: minimum purity needed for a split\n",
       "\n",
       "\n",
       "\\item \\texttt{rng}: the random number generator to use. Can be an \\texttt{Int}, which will be used to seed and create a new random number generator. Multi-threaded forests must be seeded with an \\texttt{Int}\n",
       "\n",
       "\\end{itemize}\n",
       "Implements \\texttt{fit!}, \\texttt{predict}, \\texttt{predict\\_proba}, \\texttt{get\\_classes}\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "RandomForestClassifier(; n_subfeatures::Int=-1,\n",
       "                       n_trees::Int=10,\n",
       "                       partial_sampling::Float=0.7,\n",
       "                       max_depth::Int=-1,\n",
       "                       rng=Random.GLOBAL_RNG)\n",
       "```\n",
       "\n",
       "Random forest classification. See [DecisionTree.jl's documentation](https://github.com/bensadeghi/DecisionTree.jl)\n",
       "\n",
       "Hyperparameters:\n",
       "\n",
       "  * `n_subfeatures`: number of features to consider at random per split (default: -1, sqrt(# features))\n",
       "  * `n_trees`: number of trees to train (default: 10)\n",
       "  * `partial_sampling`: fraction of samples to train each tree on (default: 0.7)\n",
       "  * `max_depth`: maximum depth of the decision trees (default: no maximum)\n",
       "  * `min_samples_leaf`: the minimum number of samples each leaf needs to have\n",
       "  * `min_samples_split`: the minimum number of samples in needed for a split\n",
       "  * `min_purity_increase`: minimum purity needed for a split\n",
       "  * `rng`: the random number generator to use. Can be an `Int`, which will be used to seed and create a new random number generator. Multi-threaded forests must be seeded with an `Int`\n",
       "\n",
       "Implements `fit!`, `predict`, `predict_proba`, `get_classes`\n"
      ],
      "text/plain": [
       "\u001b[36m  RandomForestClassifier(; n_subfeatures::Int=-1,\u001b[39m\n",
       "\u001b[36m                         n_trees::Int=10,\u001b[39m\n",
       "\u001b[36m                         partial_sampling::Float=0.7,\u001b[39m\n",
       "\u001b[36m                         max_depth::Int=-1,\u001b[39m\n",
       "\u001b[36m                         rng=Random.GLOBAL_RNG)\u001b[39m\n",
       "\n",
       "  Random forest classification. See DecisionTree.jl's documentation\n",
       "  (https://github.com/bensadeghi/DecisionTree.jl)\n",
       "\n",
       "  Hyperparameters:\n",
       "\n",
       "    •    \u001b[36mn_subfeatures\u001b[39m: number of features to consider at random per split\n",
       "        (default: -1, sqrt(# features))\n",
       "\n",
       "    •    \u001b[36mn_trees\u001b[39m: number of trees to train (default: 10)\n",
       "\n",
       "    •    \u001b[36mpartial_sampling\u001b[39m: fraction of samples to train each tree on\n",
       "        (default: 0.7)\n",
       "\n",
       "    •    \u001b[36mmax_depth\u001b[39m: maximum depth of the decision trees (default: no\n",
       "        maximum)\n",
       "\n",
       "    •    \u001b[36mmin_samples_leaf\u001b[39m: the minimum number of samples each leaf needs to\n",
       "        have\n",
       "\n",
       "    •    \u001b[36mmin_samples_split\u001b[39m: the minimum number of samples in needed for a\n",
       "        split\n",
       "\n",
       "    •    \u001b[36mmin_purity_increase\u001b[39m: minimum purity needed for a split\n",
       "\n",
       "    •    \u001b[36mrng\u001b[39m: the random number generator to use. Can be an \u001b[36mInt\u001b[39m, which will\n",
       "        be used to seed and create a new random number generator.\n",
       "        Multi-threaded forests must be seeded with an \u001b[36mInt\u001b[39m\n",
       "\n",
       "  Implements \u001b[36mfit!\u001b[39m, \u001b[36mpredict\u001b[39m, \u001b[36mpredict_proba\u001b[39m, \u001b[36mget_classes\u001b[39m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.9607843137254902\n",
       " 0.9215686274509803\n",
       " 0.9583333333333334"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_trees=12,partial_sampling=0.6,max_depth=5)\n",
    "fit!(model,features,labels)\n",
    "accuracy = cross_val_score(model, features, labels, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's pratice on an harder example (Digits NIST)\n",
    "\n",
    "And see if julia works better than python !\n",
    "In this example we'll fit a Randomn Forest classifier with the Digits NIST dataset and try to optimize its parameters with the GridSearchCV from sklearn. \n",
    "GridSearchCV is explained and the following code is adapted from [here](https://github.com/cstjean/ScikitLearn.jl/blob/master/examples/Randomized_Search.ipynb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1797-element Array{String,1}:\n",
       " \"1\"\n",
       " \"2\"\n",
       " \"3\"\n",
       " \"4\"\n",
       " \"5\"\n",
       " \"6\"\n",
       " \"7\"\n",
       " \"8\"\n",
       " \"9\"\n",
       " \"10\"\n",
       " \"1\"\n",
       " \"2\"\n",
       " \"3\"\n",
       " ⋮\n",
       " \"8\"\n",
       " \"10\"\n",
       " \"6\"\n",
       " \"5\"\n",
       " \"9\"\n",
       " \"9\"\n",
       " \"5\"\n",
       " \"10\"\n",
       " \"1\"\n",
       " \"9\"\n",
       " \"10\"\n",
       " \"9\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ScikitLearn.GridSearch: GridSearchCV\n",
    "using Printf, Statistics\n",
    "\n",
    "features, labels = load_data(\"digits\")\n",
    "\n",
    "# the data loaded are of type Array{Any}\n",
    "# cast them to concrete types for better performance\n",
    "features = float.(features)\n",
    "labels   = string.(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.9335548172757475\n",
       " 0.9499165275459098\n",
       " 0.924496644295302"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_trees=100,partial_sampling=0.7)\n",
    "fit!(model,features,labels)\n",
    "accuracy = cross_val_score(model, features, labels, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This random forest classifies quite well our data, let's try to optimize the hyperparameters. First run with these param_grid, at the end if you have time you can come back here and try with more parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#?RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 33.36 seconds for 162 candidate parameter settings.\n"
     ]
    }
   ],
   "source": [
    "param_grid = Dict(\"max_depth\"=> [3,10,100],\n",
    "                  \"n_subfeatures\"=> [1,3,10],\n",
    "                  \"min_samples_split\"=> [2,3,10],\n",
    "                  \"min_samples_leaf\"=> [1,3,10],\n",
    "                  \"n_trees\"=> [10, 100])\n",
    "\n",
    "# run grid search\n",
    "model = RandomForestClassifier(rng=2)\n",
    "grid_search = GridSearchCV(model, param_grid)\n",
    "\n",
    "start = time()\n",
    "fit!(grid_search, features, labels)\n",
    "\n",
    "@printf(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\\n\",\n",
    "time() - start, length(grid_search.grid_scores_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank:1\n",
      "Mean validation score: 0.945 (std: 0.008)\n",
      "Parameters: Dict{Symbol,Any}(:min_samples_split => 2,:n_trees => 100,:n_subfeatures => 3,:min_samples_leaf => 1,:max_depth => 100)\n",
      "\n",
      "Model with rank:2\n",
      "Mean validation score: 0.943 (std: 0.009)\n",
      "Parameters: Dict{Symbol,Any}(:min_samples_split => 3,:n_trees => 100,:n_subfeatures => 3,:min_samples_leaf => 1,:max_depth => 100)\n",
      "\n",
      "Model with rank:3\n",
      "Mean validation score: 0.942 (std: 0.004)\n",
      "Parameters: Dict{Symbol,Any}(:min_samples_split => 3,:n_trees => 100,:n_subfeatures => 3,:min_samples_leaf => 1,:max_depth => 10)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Utility function to report best scores\n",
    "function report(grid_scores, n_top=3)\n",
    "    top_scores = sort(grid_scores, by=x->x.mean_validation_score, rev=true)[1:n_top]\n",
    "    for (i, score) in enumerate(top_scores)\n",
    "        println(\"Model with rank:$i\")\n",
    "        @printf(\"Mean validation score: %.3f (std: %.3f)\\n\",\n",
    "                score.mean_validation_score,\n",
    "                std(score.cv_validation_scores))\n",
    "        println(\"Parameters: $(score.parameters)\")\n",
    "        println(\"\")\n",
    "    end\n",
    "end\n",
    "\n",
    "report(grid_search.grid_scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's compare with the Random forest from Python\n",
    "This may take a few minutes to run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV time is "
     ]
    },
    {
     "data": {
      "text/plain": [
       "114.10692858695984"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using PyCall\n",
    "py\"\"\"\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "digits = datasets.load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "param_grid = {\"max_depth\": [3, 10, 100],\n",
    "                  \"max_features\": [1, 3, 10],\n",
    "                  \"min_samples_split\": [2, 3, 10],  \n",
    "                  \"min_samples_leaf\": [1, 3, 10],   \n",
    "                  \"n_estimators\":[10, 100]}\n",
    "rf = RandomForestClassifier()\n",
    "start_time = time.time()\n",
    "clf = GridSearchCV(rf, param_grid)\n",
    "clf.fit(X,y)\n",
    "end_time = time.time()\n",
    "\"\"\"\n",
    "print(\"GridSearchCV time is \")\n",
    "py\"end_time-start_time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters are "
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dict{Any,Any} with 5 entries:\n",
       "  \"min_samples_split\" => 2\n",
       "  \"max_depth\"         => 100\n",
       "  \"min_samples_leaf\"  => 1\n",
       "  \"n_estimators\"      => 100\n",
       "  \"max_features\"      => 3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Best parameters are \")\n",
    "py\"clf.best_params_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score is "
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9421448467966573"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Best score is \")\n",
    "py\"clf.best_score_\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using python module with julia sklearn\n",
    "\n",
    "**Please restart the kernel here** otherwise there will be a conflict with the RandomClassier beacause they have the same name. This is a mix between julia and python, in fact julia import a Python object (the classifier) and use it. We'll discuss later on the perfomance of this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <class 'sklearn.ensemble._forest.RandomForestClassifier'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ScikitLearn, Printf, Statistics\n",
    "using PyCall\n",
    "using ScikitLearn.GridSearch: GridSearchCV\n",
    "@sk_import datasets: load_digits\n",
    "@sk_import ensemble: RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 75.74 seconds for 162 candidate parameter settings.\n"
     ]
    }
   ],
   "source": [
    "digits = load_digits()\n",
    "X, y = digits[\"data\"], digits[\"target\"]\n",
    "param_grid = Dict(\"max_depth\"=> [3, 10, 100],\n",
    "                  \"max_features\"=> [1, 3, 10],\n",
    "                  \"min_samples_split\"=> [2, 3, 10],  \n",
    "                  \"min_samples_leaf\"=> [1, 3, 10],   \n",
    "                  \"n_estimators\"=>[10, 100])\n",
    "\n",
    "clf = RandomForestClassifier(random_state=2)\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(clf, param_grid)\n",
    "\n",
    "start = time()\n",
    "fit!(grid_search, X, y)\n",
    "\n",
    "@printf(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\\n\",\n",
    "time() - start, length(grid_search.grid_scores_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank:1\n",
      "Mean validation score: 0.945 (std: 0.000)\n",
      "Parameters: Dict{Symbol,Any}(:max_features => 3,:min_samples_split => 2,:min_samples_leaf => 1,:n_estimators => 100,:max_depth => 10)\n",
      "\n",
      "Model with rank:2\n",
      "Mean validation score: 0.945 (std: 0.004)\n",
      "Parameters: Dict{Symbol,Any}(:max_features => 3,:min_samples_split => 3,:min_samples_leaf => 1,:n_estimators => 100,:max_depth => 10)\n",
      "\n",
      "Model with rank:3\n",
      "Mean validation score: 0.945 (std: 0.000)\n",
      "Parameters: Dict{Symbol,Any}(:max_features => 3,:min_samples_split => 2,:min_samples_leaf => 1,:n_estimators => 100,:max_depth => 100)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "function report(grid_scores, n_top=3)\n",
    "    top_scores = sort(grid_scores, by=x->x.mean_validation_score, rev=true)[1:n_top]\n",
    "    for (i, score) in enumerate(top_scores)\n",
    "        println(\"Model with rank:$i\")\n",
    "        @printf(\"Mean validation score: %.3f (std: %.3f)\\n\",\n",
    "                score.mean_validation_score,\n",
    "                std(score.cv_validation_scores))\n",
    "        println(\"Parameters: $(score.parameters)\")\n",
    "        println(\"\")\n",
    "    end\n",
    "end\n",
    "report(grid_search.grid_scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of execution time\n",
    "\n",
    "Here are the results from my computer, they could change but the order of magnitude should be the same. The best score varies slighty between methods (arround 0.94) it could be an effect of the randomness involved in the algorithm. \n",
    "\n",
    "|Method| GridSearchCV time (seconds) |\n",
    "|-----|------|\n",
    "|Julia Random Forest| 33  |\n",
    "|Python Random Forest|115 |\n",
    "| Mix Julia using Python Random Forest|75|\n",
    "\n",
    "We clearly see that Julia is very efficient when the method is purely in Julia. Even with a python module it works better than the pure python one. So far there are still few algorithms which are written in Julia, Decision trees and Randomn Forest are part of them. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other classifiers \n",
    "\n",
    "Here is a comparison of the Julia Decison Trees and Random forest against various Python classifier with the accuracy score associated : ![classifiers comparison](img/classifierspic.png)\n",
    "You could see the source code for this plot [here](https://github.com/cstjean/ScikitLearn.jl/blob/master/examples/Classifier_Comparison_Julia.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
