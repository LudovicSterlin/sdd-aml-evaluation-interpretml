{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab BE AML - Seq2Seq - THONGKHAM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rmoqGZ7dMiy"
      },
      "source": [
        "BE - AML Laurent THONGKHAM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP48f9KrSKf-"
      },
      "source": [
        "# Sequence to Sequence Learning with Neural Networks\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to2rS2sASZaT"
      },
      "source": [
        "## Problematic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcFDLexXSheT"
      },
      "source": [
        "Deep Neural Networks are very powerful tools for problem solving and parameters optimization. However, the working assumption is that both the input and the target can be embodied by vectors of fixed dimensionality. Many problems do not submit to that hypothesis. Speech recognition or machine translation are sequential problems where the length of the inputs can not be asserted beforehand.\r\n",
        "\r\n",
        "The paper *Sequence to Sequence Learning\r\n",
        "with Neural Networks* aims at designing a method to learn to map sequences to sequences.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ruxsIjvcz1o"
      },
      "source": [
        "To do so, the main idea is to use a **Long Short-Term Memory(LSTM) architecture**. One is used to read the input sequence and turn it into a vector with a fixed dimensionality while a second one will extract the output sequence from the vector. The goal is to estimate the conditional probability p(y1, ..., yT'|x1, ..., xT) with (x1, ..., xT) as the input and (y1, ..., yT') as the output sequences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PRMYtwUvk8R"
      },
      "source": [
        "![Capture.JPG](data:image/jpeg;base64,/9j/4AAQSkZJRgABAQEAYABgAAD/4REARXhpZgAATU0AKgAAAAgABAE7AAIAAAASAAAISodpAAQAAAABAAAIXJydAAEAAAAkAAAQ1OocAAcAAAgMAAAAPgAAAAAc6gAAAAgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAExhdXJlbnQgVEhPTkdLSEFNAAAFkAMAAgAAABQAABCqkAQAAgAAABQAABC+kpEAAgAAAAM5NwAAkpIAAgAAAAM5NwAA6hwABwAACAwAAAieAAAAABzqAAAACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMjAyMTowMToxMCAyMDoyMTowMAAyMDIxOjAxOjEwIDIwOjIxOjAwAAAATABhAHUAcgBlAG4AdAAgAFQASABPAE4ARwBLAEgAQQBNAAAA/+ELJGh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8APD94cGFja2V0IGJlZ2luPSfvu78nIGlkPSdXNU0wTXBDZWhpSHpyZVN6TlRjemtjOWQnPz4NCjx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iPjxyZGY6UkRGIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyI+PHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9InV1aWQ6ZmFmNWJkZDUtYmEzZC0xMWRhLWFkMzEtZDMzZDc1MTgyZjFiIiB4bWxuczpkYz0iaHR0cDovL3B1cmwub3JnL2RjL2VsZW1lbnRzLzEuMS8iLz48cmRmOkRlc2NyaXB0aW9uIHJkZjphYm91dD0idXVpZDpmYWY1YmRkNS1iYTNkLTExZGEtYWQzMS1kMzNkNzUxODJmMWIiIHhtbG5zOnhtcD0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wLyI+PHhtcDpDcmVhdGVEYXRlPjIwMjEtMDEtMTBUMjA6MjE6MDAuOTY3PC94bXA6Q3JlYXRlRGF0ZT48L3JkZjpEZXNjcmlwdGlvbj48cmRmOkRlc2NyaXB0aW9uIHJkZjphYm91dD0idXVpZDpmYWY1YmRkNS1iYTNkLTExZGEtYWQzMS1kMzNkNzUxODJmMWIiIHhtbG5zOmRjPSJodHRwOi8vcHVybC5vcmcvZGMvZWxlbWVudHMvMS4xLyI+PGRjOmNyZWF0b3I+PHJkZjpTZXEgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj48cmRmOmxpPkxhdXJlbnQgVEhPTkdLSEFNPC9yZGY6bGk+PC9yZGY6U2VxPg0KCQkJPC9kYzpjcmVhdG9yPjwvcmRmOkRlc2NyaXB0aW9uPjwvcmRmOlJERj48L3g6eG1wbWV0YT4NCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgPD94cGFja2V0IGVuZD0ndyc/Pv/bAEMABwUFBgUEBwYFBggHBwgKEQsKCQkKFQ8QDBEYFRoZGBUYFxseJyEbHSUdFxgiLiIlKCkrLCsaIC8zLyoyJyorKv/bAEMBBwgICgkKFAsLFCocGBwqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKv/AABEIAI4ClAMBIgACEQEDEQH/xAAfAAABBQEBAQEBAQAAAAAAAAAAAQIDBAUGBwgJCgv/xAC1EAACAQMDAgQDBQUEBAAAAX0BAgMABBEFEiExQQYTUWEHInEUMoGRoQgjQrHBFVLR8CQzYnKCCQoWFxgZGiUmJygpKjQ1Njc4OTpDREVGR0hJSlNUVVZXWFlaY2RlZmdoaWpzdHV2d3h5eoOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4eLj5OXm5+jp6vHy8/T19vf4+fr/xAAfAQADAQEBAQEBAQEBAAAAAAAAAQIDBAUGBwgJCgv/xAC1EQACAQIEBAMEBwUEBAABAncAAQIDEQQFITEGEkFRB2FxEyIygQgUQpGhscEJIzNS8BVictEKFiQ04SXxFxgZGiYnKCkqNTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqCg4SFhoeIiYqSk5SVlpeYmZqio6Slpqeoqaqys7S1tre4ubrCw8TFxsfIycrS09TV1tfY2dri4+Tl5ufo6ery8/T19vf4+fr/2gAMAwEAAhEDEQA/APpGiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKzvEF1eWPhvUbvS40lvLe2klhjcEh2VSQvHrjH40AaNFcBH49vLzVngsVt3tr67totIkKnM0e/F0TzyUCuRj2zXf0AFFFFABRRRQAUUUUAFFFZXie8m07wzfX1rcx20trH5yvKu5W2nOwjBOGxt45+bjnFAGrRWH4Q1S61vQf7SviI5biZz9k24Nng7fJbuXXb82f4iccYrcoAKKKKACiiigAooooAKKKwvFWs3nh6xi1S3theWkT7Lq3TAlYNhUKEkDIcqCD2YnPGCAbtFVtOF6NNg/tUwm8KAzeQCEDdwuecDpk9cZqzQAUUUUAFFFFABRRRQAUUVy/izUNatLm1tfD15ai+vfkt7Sa1MnQ/PK7BxtjUEZ464A5YCgDqKKRAwRQ5DNjkgYyfpS0AFFFFABRRRQAUUVj+IfE+n+GYIH1EybrmTy4ERfvv6b2IRf+BMAaANiiuO1vxHrNvc6NDb2psnvxO0kYt/tkiqgUrwjAc7ucEgeppLjW9duZdX/ALOuLS2Gi20bMtzasTcymLzTu+ceWmCBxkg7uTjBAOyoriLXxZqM2uWsuoh9M0u+Nv8AYd1k0qT+bGp2vMGxG/mMUAYAcDqWxWtH4vt4vEMWh6rbSWd/OSIFVlmWUDnd8hLKMd3VR2yaAOhooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACgjIwelFUtYvLiw0a7urGze+uoomaC1jODM+PlXPbJwMngdaAKGneF/Dtp/Zy6bZQKdDMsdoEcn7MZB8469SDzn1rcrw74IWvjfRPFOvw+KLYXFnqV7JJczwyq4tr4KrsGA7Mr43DIyiivcaACiiigAooooAKKKKACobm0t7yNEuoUmRJFlVXXIDKQyt9QQCPcVNXJfEtvEMvgq50/wfZvcapqP+ipKHCLaowO+VmJGMLkDHOSMA0AdJZx2ex7uwWEpeETtLDgiYlQA+R1yoXn0AqzXlvwCbxDbfDq003xHaOsMcMVxpt4HDpPbSrvVcg5DISQQccFcZAzXqVABRRRQAUUUUAFFFFABVTU4LK6094dU8s2zMm4SPtBO4Fecj+LFW68f/aDtfE2teGYNG8P23l6eZI5tQvZJAiZ8xUiiHdiXO44HG1fWgD2CisHwXd61d+EbI+KrJrPWIk8m8QsGDuvG9SvBDDDcdM47VvUAFFFFABRRRQAUUUUAFYupeHtHvdYjvrt7iG/liFujwahNbtIi5faAjrnGWNbVeDfFCLx/qPxe0HUPDenhbbSZXXTY5p1jN9IIy8+M/dUqvl/NjOMjrQB7uiCONUXcQoAG5ix/Enk06orWc3VnDOYpITLGrmKVcOmRnBHYipaACiiigAooooAKR0WSNkkUOjDDKwyCPSlooAx5vCeiyxW0cdmbRLUuYBYzSWvl7/vY8pl4OORUd54Z0HUrox3cRmnS3SKZPtUgaaHLBBMA371ch/v7gfm9622bapY5wBngZP5V4F4af4hRfH2+8Q6jpEi2V7DCt3p4nRpbWzkeRIHKhsEqYSzBcn5m45OAD2qfw3plzqSX08U0ksciSrG1zL5IdQAreVu2ZGBg7c5APWrdlptjpqyLp9nBaiVy8nkxhN7Hqxx1Puas0UAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAct4K/4+/FP/Ydl/8ARUVdTXLeCv8Aj78U/wDYdl/9FRV1NABRRRQAUUUUAFFFFABTZP8AVt9DTqbJ/q2+hoA5r4af8kp8K/8AYHtf/RK109cx8NP+SU+Ff+wPa/8Aola6egAooooAKKKKACiiigArlviR/wAiLdf9fFr/AOlMddTXLfEj/kRbr/r4tf8A0pjoA6miiigAooooAKKKKACiiigArlvEf/I9eDv+vm6/9Jnrqa5bxH/yPXg7/r5uv/SZ6AOpooooAKKKKACiiigAooooAK5aw/5K3rn/AGBNP/8AR15XU1y1h/yVvXP+wJp//o68oA6miiigAooooAKKKKACiiigAooooAKKKKACivOfBHg/T9b8D6TqWp32uzXd1brJLINevU3MepwsoA/AVu/8K70T/n517/wob/8A+PUAdTRXLf8ACu9E/wCfnXv/AAob/wD+PUf8K70T/n517/wob/8A+PUAdTRXLf8ACu9E/wCfnXv/AAob/wD+PVl6j4etfDfinwnLpV5q4N1qr286XGr3Vwkkf2O4fBSSRl+8inp2oA72iiigAoormfiLNNb/AA81eS2nlt5RCAssMhR1ywHDAgg+4oA6aiuW/wCFd6J/z869/wCFDf8A/wAeo/4V3on/AD869/4UN/8A/HqAOporlv8AhXeif8/Ovf8AhQ3/AP8AHqP+Fd6J/wA/Ovf+FDf/APx6gDqaK5b/AIV3on/Pzr3/AIUN/wD/AB6qeh6ZHofxLutPsbrUJLR9IjnMV5qE90A/nOu4ea7Y4AHFAHa0UUUAFFFcj43t/t+reFNNkubyC2vNUkS4Fpdy27SKtncOAXjZWxuRTjPagDrqK5b/AIV3on/Pzr3/AIUN/wD/AB6j/hXeif8APzr3/hQ3/wD8eoA6miuW/wCFd6J/z869/wCFDf8A/wAeo/4V3on/AD869/4UN/8A/HqAOporlv8AhXeif8/Ovf8AhQ3/AP8AHqj8AxG1XxDZC4up4bTWJIoPtd1JcOieVEdu+RixGWJ5PegDraKKKACiiuI1TSYtf+J81lf3epJbQaPDKkVpqVxarvaaUFiInXJwAOfSgDt6K5b/AIV3on/Pzr3/AIUN/wD/AB6j/hXeif8APzr3/hQ3/wD8eoAPBX/H34p/7Dsv/oqKuprkofhl4dtzKbd9aiMzmSQpr98u9yACxxNycAc+1Sf8K70T/n517/wob/8A+PUAdTRXLH4eaJj/AI+de/8AChv/AP49Uvw7uZ7z4ZeGrm7mknnm0u3eSWVyzOxjUkknkknvQB0lFFFABRRXAaN4btPEeq+IrnVbzWGkh1eWCNYNZu4ERAkZChI5FUck9u9AHf02T/Vt9DXMf8K70T/n517/AMKG/wD/AI9R/wAK70M9bnXv/Chv/wD49QA74af8kp8K/wDYHtf/AEStdPXJW/wz8O2drFbWj61BBCgSOKLX75VRQMBQBNgADtUn/Cu9E/5+de/8KG//APj1AHU0V514x8J2Og+GJtS0u+1yK6gmt9jPrt5IOZkUgq0pUggkYIr0WgAooooAKKZMSLeQjghTg/hXn3gnwZp2r+APD+pahfa9Nd3mmW888n9v3y73eJWY4EwAySeBxQB6JXLfEj/kRbr/AK+LX/0pjo/4V3on/Pzr3/hQ3/8A8eqO4+GXh27gMN2+tTxEglJdfvmUkHIODN2IB/CgDraK5b/hXeif8/Ovf+FDf/8Ax6j/AIV3on/Pzr3/AIUN/wD/AB6gDqaK4O40C28OeOfC/wDZl5q227uLiOeO51a5uEkUW0jAFZZGHBAOcdq7ygAooooAKK5f4jSSx+BbvyJ57dnntozJbytE4VriNWwykEZBIyD3pP8AhXeif8/Ovf8AhQ3/AP8AHqAOprlvEf8AyPXg7/r5uv8A0mej/hXeif8APzr3/hQ3/wD8eqOT4ZeHZpoZpX1p5YCTE7a/fFoyRglT53GQSOO1AHW0Vy3/AArvRP8An517/wAKG/8A/j1H/Cu9E/5+de/8KG//APj1AHU0VxnhzT00X4j6zplndX8lmNJsrhYry/mutsjTXSswMrsRkIo49K7OgAooooAKK4/xlajUvE3hfTZrm9htbme485bO8ltjJtgZlBaNlbAIzjNT/wDCu9E/5+de/wDChv8A/wCPUAdTXLWH/JW9c/7Amn/+jryj/hXeif8APzr3/hQ3/wD8eqNfhl4dW6e6V9aFxIixvKNfvt7KpJVSfOyQCzEDtuPrQB1tFct/wrvRP+fnXv8Awob/AP8Aj1H/AArvRP8An517/wAKG/8A/j1AHU0Vynw/V4dL1W1Nxczx2ur3UETXVw87qivhV3uSxx7muroAKKKKACiiigAooooAKKKKAOW+Gf8AyTHQP+vNK6mvN/A3jTR9H8C6Tp2pLqcF3bW4jljOkXZ2sOoyIiD+Fb//AAsbw5/z01L/AME15/8AGqAOporlJfiZ4YgiaSafUI41GWd9IuwAPc+VTv8AhY3hz/npqX/gmvP/AI1QB1Ncb8QDfjUvCH9kC2a8/ttvLF0WEf8Ax5XWclcnpn8as/8ACxvDn/PTUv8AwTXn/wAarK1LxLp/iPxV4Sg0hL+V7fVXnmMunXEKon2O5XcWdAPvOo696ANXf4+/54+G/wDv9P8A/E0b/H3/ADx8N/8Af6f/AOJrqaKAOPvX8bfYLj7dF4ZFt5TecXmnC7MfNn5emM14voE3xVk8A6sNdXf4X2HypNZ3C8Kb/l8vjcf4eZBjHSvpiuY+I8ckvw61hYYpJn8kEJEhdjhgeFGSfwoA6eiuW/4WN4c/56al/wCCa8/+NUf8LG8Of89NS/8ABNef/GqAOpork3+JnhiNo1kn1BWkbagbSLsFjgnA/dcnAJ+gNP8A+FjeHP8AnpqX/gmvP/jVAHU1wmqnXB8WX/4R5NPZ/wCxI/M+3O6jHnvjGwGtL/hY3hz/AJ6al/4Jrz/41VLQ9Wtte+Jt3fabHdm1TR44TLPZzQDf5znaPMVcnBB4oAu7/H3/ADx8N/8Af6f/AOJo3+Pv+ePhv/v9P/8AE11NFAHlnxLk8ar8PtSN0mmRvtX7O2lTXP2vz9w8vytq53bsfhnPGaxvB8nxIk1PwWfiTHaKn9qS/ZieLsn7Ddf6wL8mMYx0bOc17ZXIeObpNO1jwnqNxHcNa2mqyPO8FvJMY1azuEBKoCcbmUdO9AHX0Vy3/CxvDn/PTUv/AATXn/xqj/hY3hz/AJ6al/4Jrz/41QB1NFcp/wALM8MGYxCfUPMChin9kXe4A8A48rpwad/wsbw5/wA9NS/8E15/8aoA6mvPNCbxQNb8U/2DHpDW39tPk3skofd5MOfugjHStr/hY3hz/npqX/gmvP8A41UXw/uFvV8RX0UdwkF1rMkkJnt3hZ18qIbtrgNjIPbtQBLv8ff88fDf/f6f/wCJo3+Pv+ePhv8A7/T/APxNdTRQB4l8WJviMg0M6IsC66bphajRXlZzHt+fzQ67DFnZndxnb710fgJvFb+Obg+PI9Pj1X+xIMiwJK7POlxvzxvznO3K9MV6VXD6prVr4f8AilNdanHeLbz6NDHHLBYzTqWWeUlcxo2Dgg8+tAHcUVy3/CxvDn/PTUv/AATXn/xqj/hY3hz/AJ6al/4Jrz/41QB1NFconxM8MSlxHPqDlG2uF0i7O09cH91weRTv+FjeHP8AnpqX/gmvP/jVAHUnoa80+HreNv8AhWnhv7DFoBtv7Lt/KM0swfb5a43YXGceldEfiN4cx/rNS/8ABNef/Gql+HME1r8MPDMFzE8M0WlWySRyKVZGEaggg8g0ARb/AB9/zx8N/wDf6f8A+Jo3+Pv+ePhv/v8AT/8AxNdTRQB4J4rn+Kq/Fm2XwckLXRtU/tGO2Z208Dd8plMgAEm3+6d20LivSPhsdQNlrv8AbIthqH9sSfaRaFjF5nlRbtm7nbnpnmuzrz/RPFGm+HdV8R2urJqEUsmsSzJs0y5lV0KRgMGSMgjg9+1AHoFFct/wsbw5/wA9NS/8E15/8ao/4WP4c/56al/4Jrz/AONUAdTRXKRfEvwxPCksM+oSRyKGR00i7IYHoQfK5FO/4WN4c/56al/4Jrz/AONUAM+J3m/8K81D7PsMvmW+zfnbnz48Zx2p+/x9/wA8fDf/AH+n/wDiawvGXjHSdc8Lzadpa6lNdTz24jQ6TdJnE6E8tGAOATya9GoA5bf4+/54+G/+/wBP/wDE0b/H3/PHw3/3+n/+JrqaKAPn7wzP8Vj8QtaXw+kD+Gvtj+b/AGg0htFbP7wW7MBJjdv27Rsz7Yr1z4b/APJK/Cn/AGBrT/0SldFMM28gH90/yrz3wR430bSvh/4e07UBqcN3aaZbQTxnSLs7HWJVYZEWDgg9KAPRaK5b/hY3hz/npqX/AIJrz/41TJfiZ4Yhj3zT6hGmQNzaRdgZJwP+WXqaAOsorlv+FjeHP+empf8AgmvP/jVH/CxvDn/PTUv/AATXn/xqgCp44OojxR4R/sZbVrz7ZcbBdswj/wCPaTOSoJ6Zq3v8ff8APHw3/wB/p/8A4msq58R2HiPx34VXSUvpPs1xcSTNLp1xCiKbaRQS0iKOpA6131AHLb/H3/PHw3/3+n/+Jqnq7+Mv7Evf7Ui8NrZfZ3+0HzrgYj2nd0XPTPTmu1ooA+bvDc3xXk8Cv/wliA+HvtNr5b6tkX2PtMe3bjk84z5nboa+ka5b4kB/+EDu2SKWXy57aRkhiaRtq3EbMQqgk4AJ4Haj/hY3hz/npqX/AIJrz/41QB1NFct/wsbw5/z01L/wTXn/AMaprfEzwwkiI8+oK8hIRTpF2CxAzx+654GaAOrorlv+FjeHP+empf8AgmvP/jVH/CxvDn/PTUv/AATXn/xqgDLvTrw+Lmp/8I6mnMf7DsfO+3PIv/Le7xt2A++c+1am/wAff88fDf8A3+n/APiaqeHNTg1z4lazqOnx3X2T+yLKASz2ksAMizXTMoEiqTgOp49a7SgDlt/j7/nj4b/7/T//ABNch8UZfHCeAbtnXT47kPH9jOjy3Bu/P3DaIgFyT1yP7u6vWKKAPGvBsnxEk1/wkfiVHZK/m3H2Yx8XB/0d8+aF+TptxjnrkV7LXHeM72PSvE/hbUbuO5a1t7i4Er29tJOU3QMBkRqTyTjpVj/hY3hz/npqX/gmvP8A41QB1NFct/wsbw5/z01L/wAE15/8apg+JnhgzNEJ9QMiqGZBpF3kA5AJHldDg/kaAOsorlv+FjeHP+empf8AgmvP/jVH/CxvDn/PTUv/AATXn/xqgA8C/wCo17/sO3n/AKHXU1yfw9l+06Xq10kc0cVzrF1NF58LxMyF+G2uAQD7iusoAKKKKACiiigAooooAKKKKACiiigDyX9oO38R6t4JOk6DbKLBv9I1O8lkCqkaEbIwOrFm546bBnrXbeApdePg60t/F9q0GsWY+zXDbw6z7eFlVh1DLgnpzkYGKi+Jf/JNta/69/8A2YV1NABRRRQAUUUUAFFFFABRRRQB4T8YLXx9q/xH8Py+GLMQWumXW3TTNOqfbrryXmfjP3dkRT5sAknBwSR7bp1zLe6ZbXNxayWc00SvJby43QsRyhxwSDxkcccVz/iz/kZ/BX/YZk/9ILqupoAKKKKACiiigAooooAKKKKAPAbKP4gr+0JJ4nl0z/RJYFjk07z182PTmkZFfHTcGTzCgO7JI7nHv1ctF/yV66/7AUP/AKPlrqaACiiigAooooAKKKKACqWsXd1Y6Ld3OnWTX93FEzQWqkKZpMfKuTwATjJPAHNXaKAPDvghYeN9C8Va/F4mgW6stRvZHup4ZQ32a+Cq7ZHHDK+NwyMoo+vuNct4K/4+/FP/AGHZf/RUVdTQAUUUUAFFFFABRRRQAVyXxL/4SGfwVc6b4PtDPqeon7KJvMCLaxsDvlZiR0XIGMtkjAOK62myf6tvoaAPL/gF/wAJDafDq107xFaMLeOGK50y8WQOs1vKu8J1yGQkgggcEYyBmvUq5j4af8kp8K/9ge1/9ErXT0AFFFFABRRRQAUUUUAFeP8A7Qdl4m13wzBo+gwCLTvMjm1C8lkCKSZFSKIDkt87bjgcbV5r2CuW+JH/ACIt1/18Wv8A6Ux0AW/BVzrdx4Rsh4rszaaxCnk3a7wwkdePMDDghhhuOmSO1b1FFABRRRQAUUUUAFFFFABXg3xQtfiBqfxd0HUfDtikdvpMrrpcU1wqG+kEZknP+yrKvl/MRkDI4Ne81y3iP/kevB3/AF83X/pM9AHSWs5ubOGdoZIDLGrmKUYZCRnaR6jpUtFFABRRRQAUUUUAFFFFACM21SxzgDPAya8C8NL8Q4fj7e+IdR0pha3sUK3mnC4RpLWykeVIHIDbSUMG5gpJ+Y4GSce/Vy1h/wAlb1z/ALAmn/8Ao68oA6miiigAooooAKKKKACiiigAooooAKKKKACiiigDlviX/wAk21r/AK9//ZhXU1y3xL/5JtrX/Xv/AOzCupoAKKKKACiiigAooooAKKKKAOW8Wf8AIz+Cv+wzJ/6QXVdTXLeLP+Rn8Ff9hmT/ANILqupoAKKKKACiiigAooooAKKKKAOWi/5K9df9gKH/ANHy11NctF/yV66/7AUP/o+WupoAKKKKACiiigAooooAKKKKAOW8Ff8AH34p/wCw7L/6Kirqa5bwV/x9+Kf+w7L/AOioq6mgAooooAKKKKACiiigApsn+rb6GnU2T/Vt9DQBzXw0/wCSU+Ff+wPa/wDola6euY+Gn/JKfCv/AGB7X/0StdPQAUUUUAFFFFABRRRQAVy3xI/5EW6/6+LX/wBKY66muW+JH/Ii3X/Xxa/+lMdAHU0UUUAFFFFABRRRQAUUUUAFct4j/wCR68Hf9fN1/wCkz11Nct4j/wCR68Hf9fN1/wCkz0AdTRRRQAUUUUAFFFFABRRRQAVy1h/yVvXP+wJp/wD6OvK6muWsP+St65/2BNP/APR15QB1NFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAEF9Z2t/Yy2uoRJNbSLiSOQZVh71PXN+Nra+1PSbfSNOsku1v7hUuhK7RxC3X55FdwrYDhfL6HO/8RP4NXULfwzBY6xCYruwLWrHcXWREOEcOQN2U25OBzmgDdooooAKKKKACiiigAooooArXNtZz3NrJdpE81tKZbcvjKOVZCV99rsPoTVmvNPiVo82payxW1mlV9HngidNLa7zKXGFBAIjY9mOMetei2fmGxg8+PypfLXfHv3bGxyM98etAE1FFFABRRRQAUUUUAFFFFAFXybFNW+0FYVv5IRFuyPMaMEnHrgEk/jVquK0W0tLPWL2LW9FmuNVm1WWeK9Ng0qvGZCYWE20qmyPauCwI2HA5Ge1oAKKKKACiiigAooooAKYZolkCNIgc9FLDNc14hh8TPqWbJ2fR9g3Q6c6R3hbuN0oKFf91kYeprHXTwnxKu7q6tpVWRrT7O8mjvc7sJg/vwpEeD1JPHWgDtrWCxtLi4jtFhjmmk8+dEI3MxAG4j3AAz7VNHPFLI6RSo7xnDqrAlT7+lecabpMqLo9oukXEXiC11Uz3uotalVZN7GWTz8YcSIdoUEkblBA28SeGrCbS5bnTtPsb2505LB9t3/Zpsr5JN67YvNfasxYFju4AKZYndQB6PRXP+GovEsck/8Ab0sTWZA+zLKVa6B7+Y0arH+Cg/7xroKACiiigAooooAKQkdGI+bgA96Wua8cx3kehwappVnLe32lXkV3Dbwrl5BzHIoHr5cklAG7Y21pp9hb2VhHHDbQRiGGKPhUVRgKPoBirFee+DfDmoaT4nj0+7hl+waNas1vcMPknluBGZGB7sHSfPtKPWvQqACiiigAooooAKKKKACq97Z2uoW5tb6KOeJyGMbjIJVgwOPYgGrFcl4903UtXtrO08PrJDqm55Ir8EotsgXDqWweXDBAOcZ34+SgDrAQwypBHqKWqOiCFdBsVtbKTT4VgRUtJE2tAAMbCPbp/jV6gAooooAKKKKACiiigAqvPa2k91BczxRvNZlmikbrEWUgkemVJFWK5PxL4e/tHxLpUkaTfZbxzBqscY+SeGNGkjEnHQOAvuHZTkGgDrKKKKACiiigAooooAKKKKACqiQ2CanLdIIReTRpDJICN7KhYqp+hdv++jVuuBW203UfiKbq/wBEnthp9xttGXSJf9JnIwbh5lj27RkhctjO5j/DgA76iiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigD//Z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWF6rucNvz_b"
      },
      "source": [
        "A sequence \"A\", \"B\", \"C\", \"EOS\" (end-of-sentence token) is taken by the model to compute the probability of \"W\", \"X\", \"Y\", \"Z\", \"EOS\". The EOS token enables the model to oversee sequences of different lengths.\r\n",
        "\r\n",
        "In this notebook, we will apply the proposed method on a machine translation example and lean over the findings of the paper.\r\n",
        "\r\n",
        "The **3 important findings** of the paper are:\r\n",
        "\r\n",
        "\r\n",
        "*   the use of two different LSTMs for the input and the output sequences\r\n",
        "*   the efficiency of deep LSTMs over shallow ones\r\n",
        "*   the order reversal of the elements of the input sequence\r\n",
        "\r\n",
        "Concerning the deep LSTMs, although the authors of the paper found that deep LSTMs outperformed shallow LSTMs and decided to use an LSTM with four layers. We will only use two layers in this notebook in order to save some training time.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbVpk5upBenE"
      },
      "source": [
        "## Import libraries and data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1kBHJZvd3Iq"
      },
      "source": [
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from keras.models import Model\r\n",
        "from keras.layers import Input, LSTM, Dense\r\n",
        "import string\r\n",
        "import math"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5f9ahAO6x8L"
      },
      "source": [
        "# Parameters\r\n",
        "\r\n",
        "batch_size = 64  # Batch size for training.\r\n",
        "epochs = 100  # Number of epochs to train for.\r\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\r\n",
        "num_samples = 5000  # Number of samples to train on."
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEe71W1ld90a",
        "outputId": "7308ab86-ee2a-42ff-9668-3519c4b6ace8"
      },
      "source": [
        "!!curl -O http://www.manythings.org/anki/fra-eng.zip\r\n",
        "!!unzip fra-eng.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Archive:  fra-eng.zip',\n",
              " '  inflating: _about.txt              ',\n",
              " '  inflating: fra.txt                 ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qK9sdWsBr4Q",
        "outputId": "f05d3195-4bb5-4a44-f02e-0ff75f44f911"
      },
      "source": [
        "# Path to the data txt file.\r\n",
        "data_path = \"fra.txt\"\r\n",
        "\r\n",
        "with open(data_path, \"r\", encoding=\"utf-8\") as f:\r\n",
        "    lines = f.read().split(\"\\n\")\r\n",
        "\r\n",
        "lines[0:10]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Go.\\tVa !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)',\n",
              " 'Hi.\\tSalut !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #509819 (Aiji)',\n",
              " 'Hi.\\tSalut.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #4320462 (gillux)',\n",
              " 'Run!\\tCours\\u202f!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906331 (sacredceltic)',\n",
              " 'Run!\\tCourez\\u202f!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906332 (sacredceltic)',\n",
              " 'Who?\\tQui ?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2083030 (CK) & #4366796 (gillux)',\n",
              " 'Wow!\\tÇa alors\\u202f!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #52027 (Zifre) & #374631 (zmoo)',\n",
              " 'Fire!\\tAu feu !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1829639 (Spamster) & #4627939 (sacredceltic)',\n",
              " \"Help!\\tÀ l'aide\\u202f!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #435084 (lukaszpp) & #128430 (sysko)\",\n",
              " 'Jump.\\tSaute.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #631038 (Shishir) & #2416938 (Micsmithel)']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7qU8_y0S7GQ"
      },
      "source": [
        "# Data processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ss2fKPPB4YL"
      },
      "source": [
        "The data we will use is for english-to-french translation. As you can see, we need to clean it a bit. We will organize of dictionary of the vocabulary selected for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-YrKP--60Ng"
      },
      "source": [
        "# Vectorize the data.\r\n",
        "input_texts = []\r\n",
        "target_texts = []\r\n",
        "input_words = set()\r\n",
        "target_words = set()\r\n",
        "\r\n",
        "# This function cleans and extracts the input and target words from text_lines\r\n",
        "def extract_words(text_lines, reverse_source=True):\r\n",
        "  for line in text_lines:\r\n",
        "    input_text, target_text, _ = line.split(\"\\t\")\r\n",
        "    # We use \"tab\" as the \"start sequence\" character for the targets\r\n",
        "    # and \"\\n\" as \"end sequence\" character.\r\n",
        "    target_text = \"\\t \" + target_text + \" \\n\"\r\n",
        "\r\n",
        "    input_text = input_text.translate(str.maketrans('', '', string.punctuation))\r\n",
        "    target_text = target_text.translate(str.maketrans('', '', string.punctuation))\r\n",
        "    input_text_words = input_text.split(\" \")\r\n",
        "    target_text_words = target_text.split(\" \")\r\n",
        "\r\n",
        "    if reverse_source:\r\n",
        "      input_text = ' '.join(reversed(input_text_words))  \r\n",
        "    \r\n",
        "    input_texts.append(input_text)\r\n",
        "    target_texts.append(target_text)\r\n",
        "\r\n",
        "    for word in input_text_words:\r\n",
        "        if word not in input_words:\r\n",
        "            input_words.add(word)\r\n",
        "    for word in target_text_words:\r\n",
        "        if word not in target_words:\r\n",
        "            target_words.add(word)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7E3NYDRhEQFk"
      },
      "source": [
        "Note that in the extract_words function, we decide to **reverse the order of words in the input sentence**. This is the most important discovery of the paper as it extremely improves performances. The authors suspect that this reversal allows the model to learn short termn dependencies more easily. Usually, when the source sentence and the target sentence are concatenated together, every word is far from its translated peer. With the input reversed, even though the average distance between the pairs of words remains the same. The distance between the first words of the sentences is shortened.\r\n",
        "\r\n",
        "Contrary to what was expected, this method improves performance for short sentences but also for long sentences. Which leads to believe that reversed input sentences enables an overall better use of the LSTM's memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5u6tEyitDqnl",
        "outputId": "e86df905-cf5d-4914-9e14-03c668526ef4"
      },
      "source": [
        "# Sort the sentences by length\r\n",
        "lines.sort(key = len)\r\n",
        "short_marker = 1\r\n",
        "mid_marker = 120000\r\n",
        "long_marker = 179000\r\n",
        "print(len(lines))\r\n",
        "print()\r\n",
        "print(\"Short sentences:\")\r\n",
        "print(lines[50])\r\n",
        "print()\r\n",
        "print(\"Mid-long sentences:\")\r\n",
        "print(lines[130000])\r\n",
        "print()\r\n",
        "print(\"Long sentences:\")\r\n",
        "print(lines[179505])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "179905\n",
            "\n",
            "Short sentences:\n",
            "Help Tom.\tAidez Tom.\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203858 (CK) & #6465578 (Aiji)\n",
            "\n",
            "Mid-long sentences:\n",
            "You don't know Tom as well as I do.\tTu ne connais pas Tom aussi bien que moi.\tCC-BY 2.0 (France) Attribution: tatoeba.org #6818805 (CK) & #6818869 (Rockaround)\n",
            "\n",
            "Long sentences:\n",
            "I can't believe that you aren't at least willing to consider the possibility of other alternatives.\tJe n'arrive pas à croire que vous ne soyez pas au moins disposé à envisager d'autres possibilités.\tCC-BY 2.0 (France) Attribution: tatoeba.org #1950847 (CK) & #1951063 (sacredceltic)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PR-b7PqaHk2E"
      },
      "source": [
        "Machine translation uses very large datasets and thus costs a heavy load of computational power. The paper trained its model with a dataset of 12M sentences including 348M French words and 304M English words, and took about 10 days. Since we do not have that processing power and time, we will have to cut down on the samples of sentences.\r\n",
        "\r\n",
        "The distribution of length of the sentences should be decided accordingly to the study case. Although training long sentences is very difficult in the context of this notebook. Consider reducing the number of samples if you find yourself short of RAM.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rF6FqVB6HkDG",
        "outputId": "16d4f7be-60ae-4f0a-97f3-07b96639699f"
      },
      "source": [
        "# The distribution of length of sentences\r\n",
        "# In this case, we have a 90%/10% short/mid-long sentences ratio\r\n",
        "\r\n",
        "extract_words(lines[short_marker: short_marker + math.floor(num_samples*0.9)], False)\r\n",
        "extract_words(lines[mid_marker: mid_marker + math.floor(num_samples*0.1)], False)\r\n",
        "\r\n",
        "input_words = sorted(list(input_words))\r\n",
        "target_words = sorted(list(target_words))\r\n",
        "num_encoder_tokens = len(input_words)\r\n",
        "num_decoder_tokens = len(target_words)\r\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\r\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\r\n",
        "\r\n",
        "print(\"Number of samples:\", len(input_texts))\r\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\r\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\r\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\r\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples: 5000\n",
            "Number of unique input tokens: 2164\n",
            "Number of unique output tokens: 3632\n",
            "Max sequence length for inputs: 32\n",
            "Max sequence length for outputs: 53\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZnoAjeVNC8b",
        "outputId": "d4ca2de6-1473-4bad-f2cf-9a7f087fb21f"
      },
      "source": [
        "input_texts[-15:]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['him around gathered soon crowd A',\n",
              " 'have you do money much how About',\n",
              " 'solution no theres sure you Are',\n",
              " 'country mountainous a is Armenia',\n",
              " 'something do to tried I least At',\n",
              " 'included are lunch and Breakfast',\n",
              " 'way another in phrased be it Can',\n",
              " 'suitcase my pack me help you Can',\n",
              " 'her interest doesnt food Canned',\n",
              " 'oclock six exactly at here Come',\n",
              " 'second a for this hold you Could',\n",
              " 'yesterday yourself enjoy you Did',\n",
              " 'him to letter a write you Didnt',\n",
              " 'patio the on table a have you Do',\n",
              " 'box this open to how know you Do']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D26Kv8HX8A4D",
        "outputId": "5c043579-000c-45c5-a5b6-338790e4ceeb"
      },
      "source": [
        "target_texts[-15:]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\t Bientôt une foule se rassembla autour de lui \\n',\n",
              " '\\t Combien avezvous dargent\\u202fenviron\\u202f \\n',\n",
              " '\\t Estu sûr quil ny a pas de solution\\xa0 \\n',\n",
              " '\\t LArménie est un pays montagneux \\n',\n",
              " '\\t Au moins jai essayé de faire quelque chose \\n',\n",
              " '\\t Le petitdéjeuner et le déjeuner sont inclus \\n',\n",
              " '\\t Estce quon peut formuler ça dune autre manière\\u202f \\n',\n",
              " '\\t Peuxtu maider à faire ma valise  \\n',\n",
              " '\\t La nourriture en conserve ne lintéresse pas \\n',\n",
              " '\\t Venez ici à dixhuit heures précises \\n',\n",
              " '\\t Estce que tu pourrais tenir ça une seconde\\xa0 \\n',\n",
              " '\\t Estce que vous vous êtes bien amusé hier  \\n',\n",
              " '\\t Ne lui avezvous pas écrit une lettre\\u202f \\n',\n",
              " '\\t Disposestu dune table sur le patio  \\n',\n",
              " '\\t Savezvous comment ouvrir cette boîte  \\n']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gBmHTqVNSI9"
      },
      "source": [
        "We tokenize the words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LUEM4NL8JPX"
      },
      "source": [
        "input_token_index = dict([(word, i) for i, word in enumerate(input_words)])\r\n",
        "target_token_index = dict([(word, i) for i, word in enumerate(target_words)])\r\n",
        "\r\n",
        "encoder_input_data = np.zeros(\r\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\r\n",
        ")\r\n",
        "\r\n",
        "decoder_input_data = np.zeros(\r\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\r\n",
        ")\r\n",
        "\r\n",
        "decoder_target_data = np.zeros(\r\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\r\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2otnXVQ-IkLl"
      },
      "source": [
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\r\n",
        "    input_text_words = input_text.split(\" \")\r\n",
        "    target_text_words = target_text.split(\" \")\r\n",
        "    for t, word in enumerate(input_text_words):\r\n",
        "        encoder_input_data[i, t, input_token_index[word]] = 1.0\r\n",
        "    for t, word in enumerate(target_text_words):\r\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\r\n",
        "        decoder_input_data[i, t, target_token_index[word]] = 1.0\r\n",
        "        if t > 0:\r\n",
        "            # decoder_target_data will be ahead by one timestep\r\n",
        "            # and will not include the start word.\r\n",
        "            decoder_target_data[i, t - 1, target_token_index[word]] = 1.0\r\n",
        "    decoder_input_data[i, t + 1 :, target_token_index[\"\\n\"]] = 1.0\r\n",
        "    decoder_target_data[i, t:, target_token_index[\"\\n\"]] = 1.0"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWiC-VTAOu6E"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY6R5byWOyT4"
      },
      "source": [
        "Here, we build the model with 2 layers of LSTM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWRHW6NyewXB"
      },
      "source": [
        "# Define an input sequence and process it.\r\n",
        "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\r\n",
        "encoder = keras.layers.LSTM(latent_dim, return_state=True)\r\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs) \r\n",
        "\r\n",
        "# We discard `encoder_outputs` and only keep the states.\r\n",
        "encoder_states = [state_h, state_c]\r\n",
        "\r\n",
        "# Set up the decoder, using `encoder_states` as initial state.\r\n",
        "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\r\n",
        "\r\n",
        "# We set up the decoder to return full output sequences,\r\n",
        "# and to return internal states as well. We don't use the\r\n",
        "# return states in the training model, but we will use them in inference.\r\n",
        "decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\r\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\r\n",
        "decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\r\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\r\n",
        "\r\n",
        "# Define the model\r\n",
        "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLeNb4nzeyDK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9983bb51-b06a-4536-b780-f1ab16128431"
      },
      "source": [
        "model.compile(\r\n",
        "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\r\n",
        ")\r\n",
        "model.fit(\r\n",
        "    [encoder_input_data, decoder_input_data],\r\n",
        "    decoder_target_data,\r\n",
        "    batch_size=batch_size,\r\n",
        "    epochs=epochs,\r\n",
        "    validation_split=0.2,\r\n",
        ")\r\n",
        "# Save model\r\n",
        "model.save(\"s2s\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "63/63 [==============================] - 17s 126ms/step - loss: 2.1164 - accuracy: 0.8789 - val_loss: 0.8551 - val_accuracy: 0.9075\n",
            "Epoch 2/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 0.3368 - accuracy: 0.9516 - val_loss: 0.9630 - val_accuracy: 0.9074\n",
            "Epoch 3/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.3229 - accuracy: 0.9519 - val_loss: 0.8695 - val_accuracy: 0.9093\n",
            "Epoch 4/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 0.3142 - accuracy: 0.9522 - val_loss: 0.9056 - val_accuracy: 0.9091\n",
            "Epoch 5/100\n",
            "63/63 [==============================] - 7s 105ms/step - loss: 0.3010 - accuracy: 0.9529 - val_loss: 0.9186 - val_accuracy: 0.9094\n",
            "Epoch 6/100\n",
            "63/63 [==============================] - 7s 109ms/step - loss: 0.2924 - accuracy: 0.9535 - val_loss: 0.9280 - val_accuracy: 0.9091\n",
            "Epoch 7/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.2859 - accuracy: 0.9532 - val_loss: 0.9497 - val_accuracy: 0.9092\n",
            "Epoch 8/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 0.2766 - accuracy: 0.9541 - val_loss: 0.8342 - val_accuracy: 0.9094\n",
            "Epoch 9/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 0.2706 - accuracy: 0.9541 - val_loss: 0.8715 - val_accuracy: 0.9091\n",
            "Epoch 10/100\n",
            "63/63 [==============================] - 7s 109ms/step - loss: 0.2652 - accuracy: 0.9546 - val_loss: 0.8112 - val_accuracy: 0.9103\n",
            "Epoch 11/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.2594 - accuracy: 0.9549 - val_loss: 0.8387 - val_accuracy: 0.9104\n",
            "Epoch 12/100\n",
            "63/63 [==============================] - 7s 109ms/step - loss: 0.2532 - accuracy: 0.9554 - val_loss: 0.8120 - val_accuracy: 0.9103\n",
            "Epoch 13/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 0.2479 - accuracy: 0.9559 - val_loss: 0.8086 - val_accuracy: 0.9107\n",
            "Epoch 14/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.2434 - accuracy: 0.9563 - val_loss: 0.7981 - val_accuracy: 0.9108\n",
            "Epoch 15/100\n",
            "63/63 [==============================] - 7s 109ms/step - loss: 0.2354 - accuracy: 0.9574 - val_loss: 0.8111 - val_accuracy: 0.9114\n",
            "Epoch 16/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.2281 - accuracy: 0.9589 - val_loss: 0.8078 - val_accuracy: 0.9117\n",
            "Epoch 17/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 0.2221 - accuracy: 0.9597 - val_loss: 0.8213 - val_accuracy: 0.9119\n",
            "Epoch 18/100\n",
            "63/63 [==============================] - 7s 110ms/step - loss: 0.2179 - accuracy: 0.9598 - val_loss: 0.8214 - val_accuracy: 0.9129\n",
            "Epoch 19/100\n",
            "63/63 [==============================] - 7s 109ms/step - loss: 0.2087 - accuracy: 0.9610 - val_loss: 0.8306 - val_accuracy: 0.9128\n",
            "Epoch 20/100\n",
            "63/63 [==============================] - 7s 109ms/step - loss: 0.2034 - accuracy: 0.9615 - val_loss: 0.8111 - val_accuracy: 0.9112\n",
            "Epoch 21/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.1995 - accuracy: 0.9616 - val_loss: 0.8298 - val_accuracy: 0.9116\n",
            "Epoch 22/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 0.1961 - accuracy: 0.9619 - val_loss: 0.8153 - val_accuracy: 0.9134\n",
            "Epoch 23/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.1923 - accuracy: 0.9623 - val_loss: 0.8165 - val_accuracy: 0.9125\n",
            "Epoch 24/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 0.1862 - accuracy: 0.9636 - val_loss: 0.8121 - val_accuracy: 0.9132\n",
            "Epoch 25/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.1800 - accuracy: 0.9641 - val_loss: 0.8223 - val_accuracy: 0.9135\n",
            "Epoch 26/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.1772 - accuracy: 0.9646 - val_loss: 0.8368 - val_accuracy: 0.9138\n",
            "Epoch 27/100\n",
            "63/63 [==============================] - 7s 109ms/step - loss: 0.1717 - accuracy: 0.9653 - val_loss: 0.8320 - val_accuracy: 0.9127\n",
            "Epoch 28/100\n",
            "63/63 [==============================] - 7s 109ms/step - loss: 0.1670 - accuracy: 0.9660 - val_loss: 0.8434 - val_accuracy: 0.9132\n",
            "Epoch 29/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.1645 - accuracy: 0.9660 - val_loss: 0.8567 - val_accuracy: 0.9135\n",
            "Epoch 30/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 0.1603 - accuracy: 0.9670 - val_loss: 0.8555 - val_accuracy: 0.9140\n",
            "Epoch 31/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.1566 - accuracy: 0.9676 - val_loss: 0.8297 - val_accuracy: 0.9137\n",
            "Epoch 32/100\n",
            "63/63 [==============================] - 7s 109ms/step - loss: 0.1534 - accuracy: 0.9680 - val_loss: 0.8441 - val_accuracy: 0.9148\n",
            "Epoch 33/100\n",
            "63/63 [==============================] - 7s 109ms/step - loss: 0.1480 - accuracy: 0.9689 - val_loss: 0.8587 - val_accuracy: 0.9142\n",
            "Epoch 34/100\n",
            "63/63 [==============================] - 7s 109ms/step - loss: 0.1438 - accuracy: 0.9694 - val_loss: 0.8482 - val_accuracy: 0.9126\n",
            "Epoch 35/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.1397 - accuracy: 0.9704 - val_loss: 0.8448 - val_accuracy: 0.9135\n",
            "Epoch 36/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 0.1387 - accuracy: 0.9702 - val_loss: 0.8742 - val_accuracy: 0.9148\n",
            "Epoch 37/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.1357 - accuracy: 0.9707 - val_loss: 0.8850 - val_accuracy: 0.9132\n",
            "Epoch 38/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.1330 - accuracy: 0.9707 - val_loss: 0.8565 - val_accuracy: 0.9148\n",
            "Epoch 39/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 0.1316 - accuracy: 0.9718 - val_loss: 0.8372 - val_accuracy: 0.9148\n",
            "Epoch 40/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 0.1248 - accuracy: 0.9727 - val_loss: 0.8666 - val_accuracy: 0.9148\n",
            "Epoch 41/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 0.1246 - accuracy: 0.9727 - val_loss: 0.8716 - val_accuracy: 0.9138\n",
            "Epoch 42/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.1214 - accuracy: 0.9730 - val_loss: 0.9019 - val_accuracy: 0.9143\n",
            "Epoch 43/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.1186 - accuracy: 0.9735 - val_loss: 0.8843 - val_accuracy: 0.9144\n",
            "Epoch 44/100\n",
            "63/63 [==============================] - 7s 111ms/step - loss: 0.1174 - accuracy: 0.9739 - val_loss: 0.8671 - val_accuracy: 0.9139\n",
            "Epoch 45/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.1153 - accuracy: 0.9741 - val_loss: 0.8942 - val_accuracy: 0.9139\n",
            "Epoch 46/100\n",
            "63/63 [==============================] - 7s 110ms/step - loss: 0.1123 - accuracy: 0.9743 - val_loss: 0.8654 - val_accuracy: 0.9138\n",
            "Epoch 47/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.1082 - accuracy: 0.9755 - val_loss: 0.8747 - val_accuracy: 0.9141\n",
            "Epoch 48/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.1082 - accuracy: 0.9752 - val_loss: 0.8996 - val_accuracy: 0.9143\n",
            "Epoch 49/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.1052 - accuracy: 0.9761 - val_loss: 0.8883 - val_accuracy: 0.9139\n",
            "Epoch 50/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 0.1034 - accuracy: 0.9766 - val_loss: 0.8924 - val_accuracy: 0.9141\n",
            "Epoch 51/100\n",
            "63/63 [==============================] - 7s 109ms/step - loss: 0.1011 - accuracy: 0.9764 - val_loss: 0.8829 - val_accuracy: 0.9140\n",
            "Epoch 52/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 0.0987 - accuracy: 0.9767 - val_loss: 0.8868 - val_accuracy: 0.9136\n",
            "Epoch 53/100\n",
            "63/63 [==============================] - 7s 109ms/step - loss: 0.0968 - accuracy: 0.9773 - val_loss: 0.8979 - val_accuracy: 0.9147\n",
            "Epoch 54/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.0938 - accuracy: 0.9781 - val_loss: 0.8920 - val_accuracy: 0.9141\n",
            "Epoch 55/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.0926 - accuracy: 0.9782 - val_loss: 0.8856 - val_accuracy: 0.9136\n",
            "Epoch 56/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.0899 - accuracy: 0.9786 - val_loss: 0.8968 - val_accuracy: 0.9142\n",
            "Epoch 57/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.0876 - accuracy: 0.9796 - val_loss: 0.8986 - val_accuracy: 0.9137\n",
            "Epoch 58/100\n",
            "63/63 [==============================] - 7s 109ms/step - loss: 0.0870 - accuracy: 0.9790 - val_loss: 0.8873 - val_accuracy: 0.9142\n",
            "Epoch 59/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.0863 - accuracy: 0.9791 - val_loss: 0.9169 - val_accuracy: 0.9149\n",
            "Epoch 60/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.0840 - accuracy: 0.9799 - val_loss: 0.9186 - val_accuracy: 0.9136\n",
            "Epoch 61/100\n",
            "63/63 [==============================] - 7s 109ms/step - loss: 0.0819 - accuracy: 0.9800 - val_loss: 0.9185 - val_accuracy: 0.9140\n",
            "Epoch 62/100\n",
            "63/63 [==============================] - 7s 109ms/step - loss: 0.0797 - accuracy: 0.9806 - val_loss: 0.9184 - val_accuracy: 0.9140\n",
            "Epoch 63/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.0782 - accuracy: 0.9808 - val_loss: 0.9109 - val_accuracy: 0.9141\n",
            "Epoch 64/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.0779 - accuracy: 0.9806 - val_loss: 0.9185 - val_accuracy: 0.9141\n",
            "Epoch 65/100\n",
            "63/63 [==============================] - 7s 109ms/step - loss: 0.0744 - accuracy: 0.9817 - val_loss: 0.9378 - val_accuracy: 0.9139\n",
            "Epoch 66/100\n",
            "63/63 [==============================] - 7s 109ms/step - loss: 0.0728 - accuracy: 0.9819 - val_loss: 0.9103 - val_accuracy: 0.9141\n",
            "Epoch 67/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.0709 - accuracy: 0.9824 - val_loss: 0.9079 - val_accuracy: 0.9139\n",
            "Epoch 68/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.0687 - accuracy: 0.9827 - val_loss: 0.8886 - val_accuracy: 0.9139\n",
            "Epoch 69/100\n",
            "63/63 [==============================] - 7s 109ms/step - loss: 0.0681 - accuracy: 0.9829 - val_loss: 0.9169 - val_accuracy: 0.9138\n",
            "Epoch 70/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 0.0672 - accuracy: 0.9834 - val_loss: 0.8937 - val_accuracy: 0.9140\n",
            "Epoch 71/100\n",
            "63/63 [==============================] - 7s 109ms/step - loss: 0.0655 - accuracy: 0.9834 - val_loss: 0.8961 - val_accuracy: 0.9138\n",
            "Epoch 72/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.0635 - accuracy: 0.9841 - val_loss: 0.9213 - val_accuracy: 0.9143\n",
            "Epoch 73/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 0.0622 - accuracy: 0.9840 - val_loss: 0.9080 - val_accuracy: 0.9141\n",
            "Epoch 74/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.0617 - accuracy: 0.9843 - val_loss: 0.9136 - val_accuracy: 0.9142\n",
            "Epoch 75/100\n",
            "63/63 [==============================] - 7s 109ms/step - loss: 0.0591 - accuracy: 0.9850 - val_loss: 0.9173 - val_accuracy: 0.9141\n",
            "Epoch 76/100\n",
            "63/63 [==============================] - 7s 109ms/step - loss: 0.0585 - accuracy: 0.9850 - val_loss: 0.9177 - val_accuracy: 0.9139\n",
            "Epoch 77/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.0570 - accuracy: 0.9853 - val_loss: 0.9277 - val_accuracy: 0.9139\n",
            "Epoch 78/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.0555 - accuracy: 0.9853 - val_loss: 0.8994 - val_accuracy: 0.9137\n",
            "Epoch 79/100\n",
            "63/63 [==============================] - 7s 109ms/step - loss: 0.0548 - accuracy: 0.9859 - val_loss: 0.9402 - val_accuracy: 0.9145\n",
            "Epoch 80/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.0533 - accuracy: 0.9862 - val_loss: 0.9375 - val_accuracy: 0.9140\n",
            "Epoch 81/100\n",
            "63/63 [==============================] - 7s 109ms/step - loss: 0.0527 - accuracy: 0.9864 - val_loss: 0.9218 - val_accuracy: 0.9142\n",
            "Epoch 82/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 0.0502 - accuracy: 0.9868 - val_loss: 0.9224 - val_accuracy: 0.9140\n",
            "Epoch 83/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.0501 - accuracy: 0.9868 - val_loss: 0.9213 - val_accuracy: 0.9147\n",
            "Epoch 84/100\n",
            "63/63 [==============================] - 7s 109ms/step - loss: 0.0496 - accuracy: 0.9870 - val_loss: 0.9391 - val_accuracy: 0.9141\n",
            "Epoch 85/100\n",
            "63/63 [==============================] - 7s 109ms/step - loss: 0.0477 - accuracy: 0.9873 - val_loss: 0.9221 - val_accuracy: 0.9143\n",
            "Epoch 86/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.0471 - accuracy: 0.9877 - val_loss: 0.9089 - val_accuracy: 0.9135\n",
            "Epoch 87/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.0451 - accuracy: 0.9881 - val_loss: 0.9371 - val_accuracy: 0.9145\n",
            "Epoch 88/100\n",
            "63/63 [==============================] - 7s 109ms/step - loss: 0.0442 - accuracy: 0.9884 - val_loss: 0.9252 - val_accuracy: 0.9137\n",
            "Epoch 89/100\n",
            "63/63 [==============================] - 7s 109ms/step - loss: 0.0438 - accuracy: 0.9882 - val_loss: 0.9375 - val_accuracy: 0.9143\n",
            "Epoch 90/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 0.0432 - accuracy: 0.9885 - val_loss: 0.9529 - val_accuracy: 0.9142\n",
            "Epoch 91/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 0.0415 - accuracy: 0.9884 - val_loss: 0.9420 - val_accuracy: 0.9145\n",
            "Epoch 92/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.0409 - accuracy: 0.9890 - val_loss: 0.9254 - val_accuracy: 0.9138\n",
            "Epoch 93/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 0.0400 - accuracy: 0.9889 - val_loss: 0.9329 - val_accuracy: 0.9145\n",
            "Epoch 94/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.0391 - accuracy: 0.9895 - val_loss: 0.9186 - val_accuracy: 0.9145\n",
            "Epoch 95/100\n",
            "63/63 [==============================] - 7s 109ms/step - loss: 0.0375 - accuracy: 0.9894 - val_loss: 0.9228 - val_accuracy: 0.9134\n",
            "Epoch 96/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.0379 - accuracy: 0.9893 - val_loss: 0.9236 - val_accuracy: 0.9138\n",
            "Epoch 97/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 0.0363 - accuracy: 0.9896 - val_loss: 0.9407 - val_accuracy: 0.9141\n",
            "Epoch 98/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 0.0363 - accuracy: 0.9899 - val_loss: 0.9488 - val_accuracy: 0.9143\n",
            "Epoch 99/100\n",
            "63/63 [==============================] - 7s 109ms/step - loss: 0.0350 - accuracy: 0.9901 - val_loss: 0.9313 - val_accuracy: 0.9145\n",
            "Epoch 100/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 0.0341 - accuracy: 0.9906 - val_loss: 0.9349 - val_accuracy: 0.9139\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: s2s/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: s2s/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtgNGzbzfvhX"
      },
      "source": [
        "# Define sampling models\r\n",
        "# Restore the model and construct the encoder and decoder.\r\n",
        "model = keras.models.load_model(\"s2s\")\r\n",
        "\r\n",
        "encoder_inputs = model.input[0] \r\n",
        "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output\r\n",
        "encoder_states = [state_h_enc, state_c_enc]\r\n",
        "encoder_model = keras.Model(encoder_inputs, encoder_states)\r\n",
        "\r\n",
        "decoder_inputs = model.input[1]  \r\n",
        "decoder_state_input_h = keras.Input(shape=(latent_dim,), name=\"input_3\")\r\n",
        "decoder_state_input_c = keras.Input(shape=(latent_dim,), name=\"input_4\")\r\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\r\n",
        "decoder_lstm = model.layers[3]\r\n",
        "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\r\n",
        "    decoder_inputs, initial_state=decoder_states_inputs\r\n",
        ")\r\n",
        "decoder_states = [state_h_dec, state_c_dec]\r\n",
        "decoder_dense = model.layers[4]\r\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\r\n",
        "decoder_model = keras.Model(\r\n",
        "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\r\n",
        ")\r\n",
        "\r\n",
        "# Reverse-lookup token index to decode sequences back to something readable.\r\n",
        "reverse_input_word_index = dict((i, word) for word, i in input_token_index.items())\r\n",
        "reverse_target_word_index = dict((i, word) for word, i in target_token_index.items())"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GivvGM7QVmY"
      },
      "source": [
        "# Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOAxFpsaQYDv"
      },
      "source": [
        "Now that the model is trained, we can observe the inferences it can produce to see how the training went."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCfgVfjvOGQo"
      },
      "source": [
        "def decode_sequence(input_seq):\r\n",
        "    # Encode the input as state vectors.\r\n",
        "    states_value = encoder_model.predict(input_seq)\r\n",
        "    # Generate empty target sequence of length 1.\r\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\r\n",
        "    # Populate the first character of target sequence with the start character.\r\n",
        "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\r\n",
        "\r\n",
        "    # Sampling loop for a batch of sequences\r\n",
        "    # (here we assume a batch of size 1).\r\n",
        "    stop_condition = False\r\n",
        "    decoded_sentence = \"\"\r\n",
        "\r\n",
        "    while not stop_condition:\r\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\r\n",
        "\r\n",
        "        # Sample a token\r\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\r\n",
        "        sampled_char = reverse_target_word_index[sampled_token_index]\r\n",
        "        decoded_sentence += \" \"+sampled_char\r\n",
        "\r\n",
        "        # Stop condition: either hit max length or find stop character.\r\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\r\n",
        "            stop_condition = True\r\n",
        "\r\n",
        "        # Update the target sequence (of length 1).\r\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\r\n",
        "        target_seq[0, 0, sampled_token_index] = 1.0\r\n",
        "\r\n",
        "        # Update states\r\n",
        "        states_value = [h, c]\r\n",
        "\r\n",
        "    return decoded_sentence"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtPSwjFoOYf7",
        "outputId": "d6b8ce9a-758b-448d-91ae-019468b81fb0"
      },
      "source": [
        "# We take one sequence from the training set to try decoding\r\n",
        "sentence_marker = 4500\r\n",
        "\r\n",
        "def display_predictions(number_predictions, index):\r\n",
        "  for i in range(number_predictions):\r\n",
        "    seq_index = i + index\r\n",
        "    input_seq = encoder_input_data[seq_index : seq_index + 1]\r\n",
        "    decoded_sentence = decode_sequence(input_seq)\r\n",
        "    \r\n",
        "    input_sentence = input_texts[seq_index].split(' ')\r\n",
        "    input_sentence = ' '.join(reversed(input_sentence))\r\n",
        "\r\n",
        "    print(\"-\")\r\n",
        "    print(\"Input sentence:\", input_sentence)\r\n",
        "    print(\"Decoded sentence:\", decoded_sentence)\r\n",
        "\r\n",
        "display_predictions(10,10)\r\n",
        "display_predictions(10,4000)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: No way\n",
            "Decoded sentence:  Cest pas possible  \n",
            "\n",
            "-\n",
            "Input sentence: I try\n",
            "Decoded sentence:  Jessaye \n",
            "\n",
            "-\n",
            "Input sentence: Beat it\n",
            "Decoded sentence:  Dégage  \n",
            "\n",
            "-\n",
            "Input sentence: Here it is\n",
            "Decoded sentence:  Le voilà \n",
            "\n",
            "-\n",
            "Input sentence: Go on\n",
            "Decoded sentence:  Vasy  \n",
            "\n",
            "-\n",
            "Input sentence: Thanks\n",
            "Decoded sentence:  Merci  \n",
            "\n",
            "-\n",
            "Input sentence: Go away\n",
            "Decoded sentence:  Pars  \n",
            "\n",
            "-\n",
            "Input sentence: Go ahead\n",
            "Decoded sentence:  Passe devant  \n",
            "\n",
            "-\n",
            "Input sentence: Stop\n",
            "Decoded sentence:  Arrêtez ça  \n",
            "\n",
            "-\n",
            "Input sentence: Go on\n",
            "Decoded sentence:  Vasy  \n",
            "\n",
            "-\n",
            "Input sentence: Is this your pen\n",
            "Decoded sentence:  Estce à vous  \n",
            "\n",
            "-\n",
            "Input sentence: It happened here\n",
            "Decoded sentence:  Cest samedi \n",
            "\n",
            "-\n",
            "Input sentence: It was effective\n",
            "Decoded sentence:  Ce fut efficace \n",
            "\n",
            "-\n",
            "Input sentence: It was excessive\n",
            "Decoded sentence:  Cétait bon \n",
            "\n",
            "-\n",
            "Input sentence: Its a sunflower\n",
            "Decoded sentence:  Cest une chanson \n",
            "\n",
            "-\n",
            "Input sentence: Mail this letter\n",
            "Decoded sentence:  Prenez tout  \n",
            "\n",
            "-\n",
            "Input sentence: May I assist you\n",
            "Decoded sentence:  Je taime  \n",
            "\n",
            "-\n",
            "Input sentence: My mother is out\n",
            "Decoded sentence:  Tu es trop vieille \n",
            "\n",
            "-\n",
            "Input sentence: No ones perfect\n",
            "Decoded sentence:  On va le trouver \n",
            "\n",
            "-\n",
            "Input sentence: Pass me the wine\n",
            "Decoded sentence:  Jirai \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vnMzlOncoMR"
      },
      "source": [
        "Unsurprisingly, the model seems to provide good translations for short sentences but poor ones for longer sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nleLItDRTBhA"
      },
      "source": [
        "# Bleu scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYu4B8MGbJUR"
      },
      "source": [
        "In regards to other translation studies, we will use the BLEU score to measure the performance of our model.\r\n",
        "\r\n",
        "BLEU compares the ground truth with prediction made by either comparing words individually between the ground truth and the inference or by pairing up (up to 4 words) together to include dependencies between words. By default, the score will consider groups of 4 words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LG1OesmkSNj",
        "outputId": "8e2a7885-9616-47ae-c1cf-6c6e2821dd19"
      },
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\r\n",
        "\r\n",
        "references = []\r\n",
        "candidates = []\r\n",
        "\r\n",
        "for seq_index in range(len(encoder_input_data)):\r\n",
        "  input_seq = encoder_input_data[seq_index : seq_index + 1]\r\n",
        "  decoded_sentence = decode_sequence(input_seq).split(\" \")[1:-1]\r\n",
        "\r\n",
        "  references.append([target_texts[seq_index].split(\" \")[1:-1]])\r\n",
        "  candidates.append(decoded_sentence)\r\n",
        "\r\n",
        "#print(references)\r\n",
        "#print(candidates)\r\n",
        "\r\n",
        "score = corpus_bleu(references, candidates)\r\n",
        "print(score)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.2691350240858102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JO4UudmBdBy7"
      },
      "source": [
        "The resulting score is quite poor. But we can improve it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpJUkTbAbCvF"
      },
      "source": [
        "**Exercise:**\r\n",
        "\r\n",
        "Try improving the BLEU score by reversing the input sentence (with the extract_words function). You can also try different distribution of length sentences for training to observe the effectiveness of the model on short or long sentences.\r\n",
        "\r\n",
        "With more time, the model can also be trained with 4 layers of LSTM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg8WPWJONx7P"
      },
      "source": [
        "# Sources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTMJhJR8Q8Ee"
      },
      "source": [
        "Sequence to Sequence Learning with Neural Networks:\r\n",
        "\r\n",
        "https://proceedings.neurips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf\r\n",
        "\r\n",
        "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation:\r\n",
        "\r\n",
        "https://arxiv.org/pdf/1406.1078.pdf"
      ]
    }
  ]
}